{"cells":[{"cell_type":"code","execution_count":55,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-31T15:29:31.176433Z","iopub.status.busy":"2024-05-31T15:29:31.175634Z","iopub.status.idle":"2024-05-31T15:29:31.181992Z","shell.execute_reply":"2024-05-31T15:29:31.180754Z","shell.execute_reply.started":"2024-05-31T15:29:31.176394Z"},"trusted":true},"outputs":[],"source":["def pair_comparison(a, b):\n","    \"\"\"\n","    This function compares two values a and b.\n","    If they are equal, it returns 1.\n","    If they are not equal, it returns 0.\n","    \"\"\"\n","    if a == b:\n","        # If a is equal to b, return 1\n","        return 1\n","    else:\n","        # If a is not equal to b, return 0\n","        return 0\n"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:29:31.184818Z","iopub.status.busy":"2024-05-31T15:29:31.184370Z","iopub.status.idle":"2024-05-31T15:29:31.194575Z","shell.execute_reply":"2024-05-31T15:29:31.193600Z","shell.execute_reply.started":"2024-05-31T15:29:31.184776Z"},"trusted":true},"outputs":[],"source":["import random\n","def gram_matrix(list_of_score):\n","    \"\"\"\n","    This function computes the Gram matrix for a list of scores.\n","    \n","    The Gram matrix is a matrix of pairwise comparisons of scores.\n","    Each element [i][j] in the matrix represents the result of\n","    comparing list_of_score[i] with list_of_score[j] using the\n","    pair_comparison function.\n","    \n","    Args:\n","    - list_of_score: A list of scores\n","    \n","    Returns:\n","    - gram_matrix: The Gram matrix computed from the pairwise comparisons\n","    \"\"\"\n","    # Get the length of the list of scores\n","    n = len(list_of_score)\n","    \n","    # Initialize the Gram matrix with zeros\n","    gram_matrix = [[0 for _ in range(n)] for _ in range(n)]\n","\n","    # Iterate through each pair of scores\n","    for i in range(n):\n","        for j in range(n):\n","            # Compute the pairwise comparison using the pair_comparison function\n","            gram_matrix[i][j] = pair_comparison(list_of_score[i], list_of_score[j])\n","    \n","    # Return the computed Gram matrix\n","    return gram_matrix\n"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:29:31.196308Z","iopub.status.busy":"2024-05-31T15:29:31.195934Z","iopub.status.idle":"2024-05-31T15:29:31.212330Z","shell.execute_reply":"2024-05-31T15:29:31.211201Z","shell.execute_reply.started":"2024-05-31T15:29:31.196277Z"},"trusted":true},"outputs":[],"source":["import random\n","from collections import defaultdict\n","from itertools import cycle, islice\n","\n","def split_data_balanced_randomly(data, labels, num_groups, k):\n","    \"\"\"\n","    Split the data and labels randomly into a specified number of groups with balanced label distribution.\n","    Each group will contain exactly k samples, with items possibly repeated to ensure balanced distribution.\n","\n","    Args:\n","    - data: List of data elements\n","    - labels: List of corresponding labels\n","    - num_groups: Number of groups to split the data into\n","    - k: Number of samples in each group\n","\n","    Returns:\n","    - image_groups: List of groups containing data elements\n","    - label_groups: List of groups containing corresponding labels\n","    \"\"\"\n","    # Ensure data and labels have the same length\n","    assert len(data) == len(labels), \"Data and labels must have the same length.\"\n","    \n","    # Group data by labels\n","    label_to_data = defaultdict(list)\n","    for item, label in zip(data, labels):\n","        label_to_data[label].append(item)\n","    \n","    # Prepare the result lists\n","    image_groups = [[] for _ in range(num_groups)]\n","    label_groups = [[] for _ in range(num_groups)]\n","    \n","    # Distribute the data into the groups with repeats allowed\n","    for label, items in label_to_data.items():\n","        random.shuffle(items)  # Shuffle items within each label group\n","        item_cycle = iter(items)\n","        for group_index in range(num_groups):\n","            for _ in range(k // len(label_to_data)):\n","                try:\n","                    item = next(item_cycle)\n","                except StopIteration:\n","                    # If we run out of items for a label, shuffle and start again\n","                    random.shuffle(items)\n","                    item_cycle = iter(items)\n","                    item = next(item_cycle)\n","                image_groups[group_index].append(item)\n","                label_groups[group_index].append(label)\n","\n","    return image_groups, label_groups\n","\n","def split_data_randomly(data, labels, num_groups, k):\n","    \"\"\"\n","    Split the data and labels randomly into a specified number of groups with each group containing exactly k samples.\n","    Items can be repeated within and across groups to ensure balanced distribution.\n","\n","    Args:\n","    - data: List of data elements\n","    - labels: List of corresponding labels\n","    - num_groups: Number of groups to split the data into\n","    - k: Number of samples in each group\n","\n","    Returns:\n","    - image_groups: List of groups containing data elements\n","    - label_groups: List of groups containing corresponding labels\n","    \"\"\"\n","    # Ensure data and labels have the same length\n","    assert len(data) == len(labels), \"Data and labels must have the same length.\"\n","    \n","    # Shuffle data and labels together\n","    combined_data = list(zip(data, labels))\n","    random.shuffle(combined_data)\n","    \n","    # Prepare the result lists\n","    image_groups = [[] for _ in range(num_groups)]\n","    label_groups = [[] for _ in range(num_groups)]\n","    \n","    # Fill each group with k samples, allowing repetition\n","    for i in range(num_groups):\n","        for j in range(k):\n","            item = combined_data[random.randint(0, len(combined_data) - 1)]\n","            image_groups[i].append(item[0])\n","            label_groups[i].append(item[1])\n","    \n","    return image_groups, label_groups"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:29:31.214946Z","iopub.status.busy":"2024-05-31T15:29:31.214612Z","iopub.status.idle":"2024-05-31T15:29:31.235940Z","shell.execute_reply":"2024-05-31T15:29:31.234710Z","shell.execute_reply.started":"2024-05-31T15:29:31.214918Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","import pandas as pd\n","import cv2\n","import os\n","import torch\n","from PIL import Image\n","import pydicom\n","\n","from torchvision import transforms\n","from typing import Tuple\n","\n","mean = [0.6821, 0.4575, 0.2626]\n","std  = [0.1324, 0.1306, 0.1022]\n","data_transforms = {\n","    'training': transforms.Compose([\n","        transforms.Resize((450,200)),\n","        transforms.RandomHorizontalFlip(p=0.3),\n","        transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(),]),p=0.3),\n","        transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=3),]),p=0.3),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize((450,200)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize((450,200)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","}\n","\n","class SeveritySimilarityDataset(Dataset):\n","    def __init__(self, annotation_file_path: str, dataset_dir: str, phase: str = \"training\", num_per_cluster: int = 5, num_group: int = 10000, input_size: Tuple[int] = (224, 224), transforms=None) -> None:\n","        \"\"\"\n","        Dataset class for severity similarity task.\n","\n","        Args:\n","        - annotation_df: DataFrame containing annotations\n","        - dataset_dir: Directory containing image data\n","        - phase: Phase of the dataset (e.g., \"training\", \"validation\", \"testing\")\n","        - num_per_cluster: Number of images per cluster\n","        \"\"\"\n","        super(SeveritySimilarityDataset, self).__init__()\n","        self.dataset_dir = dataset_dir\n","        annotation_df = pd.read_csv(annotation_file_path)\n","        # Filter data based on the specified phase\n","        data = annotation_df[annotation_df[\"split\"] == phase]\n","        # Concatenate study_id and image_id to get image paths\n","        image_paths_df = data[\"study_id\"] + \"/\" + data[\"image_id\"] +\".png\"\n","        image_paths = image_paths_df.tolist()\n","        self.transforms = data_transforms[phase] if transforms == None else transforms\n","        # Get labels\n","        labels_df = data[\"breast_birads\"]\n","        labels = labels_df.to_list()\n","\n","        # Split data into clusters\n","        if phase != \"test\":\n","            self.image_cluster_list, self.label_cluster_list = split_data_balanced_randomly(image_paths, labels, num_group, num_per_cluster)\n","        else:\n","            self.image_cluster_list, self.label_cluster_list = split_data_randomly(image_paths, labels, num_group, num_per_cluster)\n","\n","\n","        self.input_size = input_size\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the number of clusters in the dataset.\n","        \"\"\"\n","        return len(self.label_cluster_list)\n","    \n","    def __getitem__(self, index):\n","        \"\"\"\n","        Retrieves a cluster of images and its corresponding label cluster.\n","\n","        Args:\n","        - index: Index of the cluster to retrieve\n","\n","        Returns:\n","        - images: List of images in the cluster\n","        - gram_matrix: Gram matrix computed from the label cluster\n","        \"\"\"\n","        image_cluster = self.image_cluster_list[index]\n","        label_cluster = self.label_cluster_list[index]\n","        images = []\n","        for image_path in image_cluster:\n","            abs_image_path = os.path.join(self.dataset_dir, image_path)\n","            \n","            # Read and preprocess image\n","            image =  self._read_image(os.path.join(self.dataset_dir,image_path), self.input_size) # Transpose image tensor\n","            images.append(image)\n","        # # Compute Gram matrix\n","        gram_matrix_ = gram_matrix(label_cluster)\n","        return  images, torch.tensor(gram_matrix_).to(torch.float)\n","    \n","    def _read_image(self, filepath, new_size):\n","        image_pil = Image.open(filepath)\n","        \n","        # Kiểm tra chế độ của ảnh\n","        if image_pil.mode != 'L':\n","            image_pil = image_pil.convert('L')  # Chuyển đổi sang chế độ 'L' (grayscale) nếu cần thiết\n","        \n","        # Tạo ảnh RGB từ ảnh đơn kênh bằng cách sao chép giá trị của kênh đó vào cả ba kênh\n","        image_pil = Image.merge('RGB', (image_pil, image_pil, image_pil))\n","        \n","    \n","        resized_image = self.transforms(image_pil)\n","        resized_image = resized_image.to(torch.float)\n","        \n","        return resized_image\n"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:29:31.408967Z","iopub.status.busy":"2024-05-31T15:29:31.407806Z","iopub.status.idle":"2024-05-31T15:29:31.423510Z","shell.execute_reply":"2024-05-31T15:29:31.422295Z","shell.execute_reply.started":"2024-05-31T15:29:31.408901Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","import torch\n","import torchvision.models as models\n","import timm\n","\n","class Setting_2_model(nn.Module):\n","    def __init__(self, model_name: str, embed_dim: int):\n","        \"\"\"\n","        A custom model for Setting 2, which uses different pre-trained models\n","        based on the specified `model_name`.\n","\n","        Args:\n","        - model_name: Name of the pre-trained model to be used\n","        - embed_dim: Dimension of the output embeddings\n","        \"\"\"\n","        super(Setting_2_model, self).__init__()\n","        self.model_name = model_name\n","        # Load the specified pre-trained model\n","        if model_name.startswith('resnet'):\n","            if model_name == 'resnet50':\n","                self.model = models.resnet50(pretrained=True)\n","            elif model_name == 'resnet101':\n","                self.model = models.resnet101(pretrained=True)\n","            elif model_name == 'resnet152':\n","                self.model = models.resnet152(pretrained=True)\n","            else:\n","                raise ValueError(f\"Unsupported ResNet model: {model_name}\")\n","                \n","            num_features = self.model.fc.in_features\n","            self.model.fc = nn.Linear(num_features, embed_dim)\n","        \n","        elif model_name.startswith('densenet'):\n","            if model_name == 'densenet121':\n","                self.model = models.densenet121(pretrained=True)\n","            else:\n","                raise ValueError(f\"Unsupported DenseNet model: {model_name}\")\n","                \n","            num_features = self.model.classifier.in_features\n","            self.model.classifier = nn.Linear(num_features, embed_dim)\n","        \n","        elif model_name.startswith('vit'):\n","            self.model = timm.create_model(model_name, pretrained=True)\n","\n","            num_features = self.model.head.in_features\n","            self.model.head = nn.Linear(num_features, embed_dim)\n","        \n","        else:\n","            raise ValueError(f\"Unsupported model: {model_name}\")\n","    \n","    def forward(self, images, device):\n","        \"\"\"\n","        Forward pass of the model.\n","\n","        Args:\n","        - images: A list of input images\n","\n","        Returns:\n","        - gram_matrix: The Gram matrix computed from the embeddings\n","        \"\"\"\n","        embeddings = []\n","        # Iterate over the list of input images\n","        for image in images:\n","            # Pass the image through the pre-trained model\n","            image = image.to(device)\n","            image_embedding = self.model(image)\n","            # Append the embedding to the list\n","            embeddings.append(image_embedding)\n","        # Stack the embeddings along a new dimension\n","        embeddings_tensor = torch.stack(embeddings, dim=1)\n","        # Normalize the embeddings\n","        embeddings_normalized = torch.nn.functional.normalize(embeddings_tensor, p=2, dim=2)\n","\n","        # Compute the Gram matrix\n","        gram_matrix = torch.matmul(embeddings_normalized, embeddings_normalized.transpose(1, 2))\n","        return gram_matrix.to(device)\n","\n"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:29:31.426118Z","iopub.status.busy":"2024-05-31T15:29:31.425737Z","iopub.status.idle":"2024-05-31T15:29:31.439985Z","shell.execute_reply":"2024-05-31T15:29:31.438771Z","shell.execute_reply.started":"2024-05-31T15:29:31.426088Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ContrastiveLoss(nn.Module):\n","    def __init__(self, margin=2.0):\n","        \"\"\"\n","        Contrastive Loss function for computing the loss between predicted\n","        and ground truth Gram matrices.\n","\n","        Args:\n","        - margin: Margin value for the loss calculation\n","        \"\"\"\n","        super(ContrastiveLoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, gram_matrix_predicted, gram_matrix_ground_truth):\n","        \"\"\"\n","        Forward pass of the Contrastive Loss function.\n","\n","        Args:\n","        - gram_matrix_predicted: Predicted Gram matrix\n","        - gram_matrix_ground_truth: Ground truth Gram matrix\n","\n","        Returns:\n","        - loss: Contrastive Learning Loss\n","        \"\"\"\n","        loss = (1-gram_matrix_ground_truth )* pow(gram_matrix_predicted, 2)\n","        + (gram_matrix_ground_truth) * pow(torch.clamp(self.margin - gram_matrix_predicted, min=0.0), 2)\n","        return torch.mean(loss)\n"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:29:31.441932Z","iopub.status.busy":"2024-05-31T15:29:31.441511Z","iopub.status.idle":"2024-05-31T15:29:31.466310Z","shell.execute_reply":"2024-05-31T15:29:31.465137Z","shell.execute_reply.started":"2024-05-31T15:29:31.441893Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torch.optim import lr_scheduler\n","\n","def train_model(model, train_dataset, val_dataset, checkpoint_folder, num_epochs=10, batch_size=32, learning_rate=0.001):\n","    \"\"\"\n","    Train the model using the provided datasets.\n","\n","    Args:\n","    - model: The model to be trained\n","    - train_dataset: Dataset for training\n","    - val_dataset: Dataset for validation\n","    - checkpoint_folder: Folder to store checkpoints\n","    - num_epochs: Number of epochs for training\n","    - batch_size: Batch size for training\n","    - learning_rate: Learning rate for optimization\n","\n","    Returns:\n","    - model: Trained model\n","    - train_losses: List of training losses\n","    - val_losses: List of validation losses\n","    \"\"\"\n","    # Create the checkpoint folder if it doesn't exist\n","    if not os.path.exists(checkpoint_folder):\n","        os.makedirs(checkpoint_folder)\n","    device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n","    print(f\"Device: {device}\")\n","    # Define data loaders for training and validation\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Define loss function and optimizer\n","    criterion = ContrastiveLoss()\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.0)\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","    # Lists to store training and validation losses\n","    train_losses = []\n","    val_losses = []\n","\n","    # Variables to keep track of the best model and its performance\n","    best_val_loss = float('inf')\n","    best_model_state = None\n","\n","    model = model.to(device)\n","    print(\"Training started...\")\n","    for epoch in range(num_epochs):\n","        print(\"*\"*100)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n","        model.train()\n","        running_train_loss = 0.0\n","        for i, (images, labels) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            # Forward pass\n","            labels = labels.to(device)\n","            outputs = model(images, device)\n","            # Compute loss\n","            loss = criterion(outputs, labels)\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","            running_train_loss += loss.item()\n","\n","            if i % 500 == 0:\n","                print(f\"\\t Batch [{i}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n","        \n","        # Compute average training loss for the epoch\n","        epoch_train_loss = running_train_loss / len(train_loader)\n","        train_losses.append(epoch_train_loss)\n","\n","        # Validation loop\n","        model.eval()\n","        running_val_loss = 0.0\n","        with torch.no_grad():\n","            for i, (images, labels) in enumerate(val_loader):\n","                labels = labels.to(device)\n","                outputs = model(images, device)\n","                loss = criterion(outputs, labels)\n","                running_val_loss += loss.item()\n","\n","                if i % 50 == 0:\n","                    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Batch [{i}/{len(val_loader)}], Val Loss: {loss.item():.4f}\")\n","        \n","        # Compute average validation loss for the epoch\n","        epoch_val_loss = running_val_loss / len(val_loader)\n","        val_losses.append(epoch_val_loss)\n","\n","        # Save the model checkpoint for every epoch (last model)\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_loss': epoch_val_loss\n","        }, os.path.join(checkpoint_folder, 'last.pt'))\n","\n","        # Save the best model checkpoint based on validation loss\n","        if epoch_val_loss < best_val_loss:\n","            best_val_loss = epoch_val_loss\n","            best_model_state = model.state_dict()\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': best_model_state,\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_loss': best_val_loss\n","            }, os.path.join(checkpoint_folder, 'best.pt'))\n","\n","        # Print progress\n","        print(f\"Validation, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n","        print(\"*\"*100)\n","        scheduler.step()\n","    print(\"Training completed.\")\n","\n","    return model, train_losses, val_losses\n","\n","\n","def test_model(model, test_dataset, batch_size=32):\n","    \"\"\"\n","    Evaluate the model on the test dataset.\n","\n","    Args:\n","    - model: The trained model to be evaluated\n","    - test_dataset: Dataset for testing\n","    - batch_size: Batch size for testing\n","\n","    Returns:\n","    - test_loss: Test loss\n","    \"\"\"\n","    # Define data loader for testing\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Define loss function\n","    criterion = nn.MSELoss()\n","\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Initialize variables for computing test loss\n","    running_test_loss = 0.0\n","    num_samples = 0\n","\n","    print(\"Testing started...\")\n","    with torch.no_grad():\n","        for images, labels in tqdm(test_loader):\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            running_test_loss += loss.item() * images.size(0)\n","            num_samples += images.size(0)\n","\n","    # Compute test loss\n","    test_loss = running_test_loss / num_samples\n","\n","    print(f\"Test Loss: {test_loss:.4f}\")\n","    print(\"Testing completed.\")\n","\n","    return test_loss"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:29:31.468179Z","iopub.status.busy":"2024-05-31T15:29:31.467715Z","iopub.status.idle":"2024-05-31T15:29:31.481877Z","shell.execute_reply":"2024-05-31T15:29:31.480547Z","shell.execute_reply.started":"2024-05-31T15:29:31.468141Z"},"trusted":true},"outputs":[],"source":["config = {\n","    \"annotation_data_path\": \"/kaggle/input/mammo-224-224-ver2/split_data.csv\",\n","    \"image_folder_path\": \"/kaggle/input/mammo-224-224-ver2/Processed_Images\",\n","    \"model_encoder\": \"resnet50\",\n","    \"embedding_dim\": 512, \n","    \"learning_rate\": 1e-2,\n","    \"num_epoch\": 20,\n","    \"batch_size\": 8,\n","    \"num_per_cluster\": 5,\n","    \"num_groups\": 10000,\n","    \"checkpoint_folder\": \"/kaggle/working/weights_setting2/resnet50BasedModel\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:29:31.485181Z","iopub.status.busy":"2024-05-31T15:29:31.484664Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Device: cpu\n","Training started...\n","****************************************************************************************************\n","Epoch [1/50]:\n","\t Batch [0/2500], Train Loss: 121.0950\n"]}],"source":["train_dataset = SeveritySimilarityDataset(annotation_file_path=config[\"annotation_data_path\"],\n","                                    dataset_dir=config[\"image_folder_path\"],\n","                                    phase=\"training\",\n","                                    num_per_cluster=config[\"num_per_cluster\"],\n","                                    num_group = config[\"num_groups\"],\n","                                    input_size=(224, 224)\n","                                    )\n","\n","valid_dataset = SeveritySimilarityDataset(annotation_file_path=config[\"annotation_data_path\"],\n","                                    dataset_dir=config[\"image_folder_path\"],\n","                                    phase=\"valid\",\n","                                    num_per_cluster=config[\"num_per_cluster\"],\n","                                    num_group = config[\"num_groups\"]//100,\n","                                    input_size=(224, 224)\n","                                    )\n","model = Setting_2_model(model_name=config[\"model_encoder\"],\n","                        embed_dim=config[\"embedding_dim\"]\n","                        )\n","\n","\n","train_model(model=model, train_dataset=train_dataset,\n","            val_dataset=valid_dataset, num_epochs=config[\"num_epoch\"],\n","            batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n","            checkpoint_folder=config[\"checkpoint_folder\"]\n","            )"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5121102,"sourceId":8566029,"sourceType":"datasetVersion"}],"dockerImageVersionId":30715,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
