{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as opt\n",
    "import timm\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import pydicom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"Label\": ['BI-RADS 1', 'BI-RADS 2', 'BI-RADS 3', 'BI-RADS 4', 'BI-RADS 5'],\n",
    "    \"extraction_feature_model_name\": \"resnet50\",\n",
    "    \"extraction_feature_model_path\": \"best.pt\",\n",
    "    \"embedding_dim\": 512,\n",
    "    \"annotation_file_path\": \"data.csv\",\n",
    "    \"dataset_dir\": \"data\",\n",
    "    \"phase\": \"training\",\n",
    "    \"input_size\": (224,224),\n",
    "    \"batch_size\": 2,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_epoch\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BI-RADS 2', 'BI-RADS 1', 'BI-RADS 3', 'BI-RADS 4', 'BI-RADS 5'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"breast-level_annotations.csv\")\n",
    "df[\"breast_birads\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class SeverityLevelDataset(Dataset):\n",
    "    def __init__(self, annotation_file_path: str, dataset_dir: str, phase: str = \"training\", input_size: Tuple = (224, 224), label_list: List = [\"\"]):\n",
    "        super(SeverityLevelDataset, self).__init__()\n",
    "        self.dataset_dir = dataset_dir\n",
    "        annotation_df = pd.read_csv(annotation_file_path)\n",
    "        # Filter data based on the specified phase\n",
    "        data = annotation_df[annotation_df[\"split\"] == phase]\n",
    "        # Concatenate study_id and image_id to get image paths\n",
    "        image_paths_df = data[\"study_id\"] + \"/\" + data[\"image_id\"] +\".dicom\"\n",
    "        self.image_path_list = image_paths_df.tolist()\n",
    "    \n",
    "        # Get labels\n",
    "        labels_df = data[\"breast_birads\"]\n",
    "        self.label_name_list = labels_df.to_list()\n",
    "        self.input_size = input_size\n",
    "        self.label_id_list = label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_name_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_path_list[index]\n",
    "        image_tensor = self._read_resize_dicom(os.path.join(self.dataset_dir, image_path), self.input_size)\n",
    "\n",
    "        label_name = self.label_name_list[index]\n",
    "        label = self.label_id_list.index(label_name)\n",
    "        return image_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def _read_resize_dicom(self, filepath, new_size):\n",
    "         # Đọc file DICOM\n",
    "        dicom_data = pydicom.dcmread(filepath)\n",
    "        \n",
    "        # Chuyển đổi dữ liệu DICOM thành mảng numpy\n",
    "        image_array = dicom_data.pixel_array\n",
    "        \n",
    "        # Chuyển đổi mảng numpy thành ảnh PIL\n",
    "        image_pil = Image.fromarray(image_array)\n",
    "        \n",
    "        # Kiểm tra chế độ của ảnh\n",
    "        if image_pil.mode != 'L':\n",
    "            image_pil = image_pil.convert('L')  # Chuyển đổi sang chế độ 'L' (grayscale) nếu cần thiết\n",
    "        \n",
    "        # Tạo ảnh RGB từ ảnh đơn kênh bằng cách sao chép giá trị của kênh đó vào cả ba kênh\n",
    "        image_pil = Image.merge('RGB', (image_pil, image_pil, image_pil))\n",
    "        \n",
    "        # Resize ảnh\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(new_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        resized_image = transform(image_pil)\n",
    "        resized_image = resized_image.to(torch.float)\n",
    "        \n",
    "        return resized_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Extractionmodel(nn.Module):\n",
    "    def __init__(self, model_name: str, embed_dim: int):\n",
    "        \"\"\"\n",
    "        A custom model for Setting 2, which uses different pre-trained models\n",
    "        based on the specified `model_name`.\n",
    "\n",
    "        Args:\n",
    "        - model_name: Name of the pre-trained model to be used\n",
    "        - embed_dim: Dimension of the output embeddings\n",
    "        \"\"\"\n",
    "        super(Extractionmodel, self).__init__()\n",
    "\n",
    "        # Load the specified pre-trained model\n",
    "        if model_name.startswith('resnet'):\n",
    "            if model_name == 'resnet50':\n",
    "                self.model = models.resnet50(pretrained=True)\n",
    "            elif model_name == 'resnet101':\n",
    "                self.model = models.resnet101(pretrained=True)\n",
    "            elif model_name == 'resnet152':\n",
    "                self.model = models.resnet152(pretrained=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ResNet model: {model_name}\")\n",
    "                \n",
    "            num_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(num_features, embed_dim)\n",
    "        \n",
    "        elif model_name.startswith('densenet'):\n",
    "            if model_name == 'densenet121':\n",
    "                self.model = models.densenet121(pretrained=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported DenseNet model: {model_name}\")\n",
    "                \n",
    "            num_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(num_features, embed_dim)\n",
    "        \n",
    "        elif model_name.startswith('vit'):\n",
    "            self.model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "            num_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(num_features, embed_dim)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class SeverityClassificationModel(nn.Module):\n",
    "    def __init__(self, feature_extraction_model, num_class):\n",
    "        super(SeverityClassificationModel, self).__init__()\n",
    "        self.feature_extractor = feature_extraction_model\n",
    "        self.fc = nn.Linear(512, num_class)\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature_extractor(image)\n",
    "        out = self.fc(feature)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackson/anaconda3/envs/Paper/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jackson/anaconda3/envs/Paper/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0442,  0.0592,  0.0504, -0.0100,  0.0014]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "checkpoint = torch.load(config[\"extraction_feature_model_path\"], map_location=torch.device(\"cpu\"))\n",
    "extractionmodel = Extractionmodel(model_name=config[\"extraction_feature_model_name\"], embed_dim=config[\"embedding_dim\"])\n",
    "extractionmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "cls_model = SeverityClassificationModel(extractionmodel, 5)\n",
    "\n",
    "## dataset test\n",
    "datset = SeverityLevelDataset(annotation_file_path=config[\"annotation_file_path\"], \n",
    "                              dataset_dir=config[\"dataset_dir\"], \n",
    "                              phase=config[\"phase\"], \n",
    "                              input_size=config[\"input_size\"], \n",
    "                              label_list=config[\"Label\"]\n",
    "                              )\n",
    "\n",
    "sample_0 = datset[0]\n",
    "out = cls_model(sample_0[0].unsqueeze(0))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs, device, save_best_model=True, save_last_model=True):\n",
    "    best_loss = float('inf')  # Khởi tạo best_loss với giá trị vô cùng lớn\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Chuyển mô hình sang chế độ training\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)  # Chuyển dữ liệu vào thiết bị (ví dụ: GPU)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Đặt gradients về zero\n",
    "\n",
    "            # Tính toán output của mô hình và loss\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Lan truyền ngược và cập nhật trọng số\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Cập nhật tổng loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Tính loss trung bình cho mỗi epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # In ra loss của epoch hiện tại\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # Kiểm tra model với dữ liệu validation\n",
    "        model.eval()  # Chuyển mô hình sang chế độ evaluation\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Tính loss trung bình cho dữ liệu validation\n",
    "        valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "        print(f'Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "        # Lưu lại model tốt nhất\n",
    "        if save_best_model and valid_loss < best_loss:\n",
    "            torch.save(model.state_dict(), '_best.pt')\n",
    "            best_loss = valid_loss\n",
    "            print('Best model saved.')\n",
    "\n",
    "    # Lưu lại model cuối cùng\n",
    "    if save_last_model:\n",
    "        torch.save(model.state_dict(), '_last.pt')\n",
    "        print('Last model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def test_model(model, criterion, test_loader, device):\n",
    "    model.eval()  # Chuyển mô hình sang chế độ evaluation\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Tính loss trung bình và độ chính xác trên dữ liệu kiểm tra\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackson/anaconda3/envs/Paper/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jackson/anaconda3/envs/Paper/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 1.0714\n",
      "Validation Loss: 4.5371\n",
      "Best model saved.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(config[\"extraction_feature_model_path\"], map_location=torch.device(\"cpu\"))\n",
    "extractionmodel = Extractionmodel(model_name=config[\"extraction_feature_model_name\"], embed_dim=config[\"embedding_dim\"])\n",
    "extractionmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "cls_model = SeverityClassificationModel(extractionmodel, 5)\n",
    "\n",
    "## dataset test\n",
    "train_datset = SeverityLevelDataset(annotation_file_path=config[\"annotation_file_path\"], \n",
    "                              dataset_dir=config[\"dataset_dir\"], \n",
    "                              phase=config[\"phase\"], \n",
    "                              input_size=config[\"input_size\"], \n",
    "                              label_list=config[\"Label\"]\n",
    "                              )\n",
    "\n",
    "valid_datset = SeverityLevelDataset(annotation_file_path=config[\"annotation_file_path\"], \n",
    "                              dataset_dir=config[\"dataset_dir\"], \n",
    "                              phase=\"valid\", \n",
    "                              input_size=config[\"input_size\"], \n",
    "                              label_list=config[\"Label\"]\n",
    "                              )\n",
    "test_datset = SeverityLevelDataset(annotation_file_path=config[\"annotation_file_path\"], \n",
    "                              dataset_dir=config[\"dataset_dir\"], \n",
    "                              phase=\"testing\", \n",
    "                              input_size=config[\"input_size\"], \n",
    "                              label_list=config[\"Label\"]\n",
    "                              )\n",
    "train_loader = DataLoader(train_datset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_datset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_datset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = opt.AdamW(cls_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "train_model(cls_model, criterion, optimizer, train_loader, valid_loader, config[\"num_epoch\"], torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), True, True)\n",
    "# test_model(cls_model, criterion, test_loader, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
