{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8586893,"sourceType":"datasetVersion","datasetId":5135991},{"sourceId":60611,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":50633}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def pair_comparison(a, b):\n    \"\"\"\n    This function compares two values a and b.\n    If they are equal, it returns 0.\n    If they are not equal, it returns 1.\n    \"\"\"\n    if a == b:\n        # If a is equal to b, return 0\n        return 0\n    else:\n        # If a is not equal to b, return 1\n        return 1\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\ndef gram_matrix(list_of_score):\n    \"\"\"\n    This function computes the Gram matrix for a list of scores.\n    \n    The Gram matrix is a matrix of pairwise comparisons of scores.\n    Each element [i][j] in the matrix represents the result of\n    comparing list_of_score[i] with list_of_score[j] using the\n    pair_comparison function.\n    \n    Args:\n    - list_of_score: A list of scores\n    \n    Returns:\n    - gram_matrix: The Gram matrix computed from the pairwise comparisons\n    \"\"\"\n    # Get the length of the list of scores\n    n = len(list_of_score)\n    \n    # Initialize the Gram matrix with zeros\n    gram_matrix = [[0 for _ in range(n)] for _ in range(n)]\n\n    # Iterate through each pair of scores\n    for i in range(n):\n        for j in range(n):\n            # Compute the pairwise comparison using the pair_comparison function\n            gram_matrix[i][j] = pair_comparison(list_of_score[i], list_of_score[j])\n    \n    # Return the computed Gram matrix\n    return gram_matrix\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom collections import defaultdict\nfrom itertools import cycle, islice\n\ndef split_data_balanced_randomly(data, labels, num_groups, k):\n    \"\"\"\n    Split the data and labels randomly into a specified number of groups with balanced label distribution.\n    Each group will contain exactly k samples, with items possibly repeated to ensure balanced distribution.\n\n    Args:\n    - data: List of data elements\n    - labels: List of corresponding labels\n    - num_groups: Number of groups to split the data into\n    - k: Number of samples in each group\n\n    Returns:\n    - image_groups: List of groups containing data elements\n    - label_groups: List of groups containing corresponding labels\n    \"\"\"\n    # Ensure data and labels have the same length\n    assert len(data) == len(labels), \"Data and labels must have the same length.\"\n    \n    # Group data by labels\n    label_to_data = defaultdict(list)\n    for item, label in zip(data, labels):\n        label_to_data[label].append(item)\n    \n    # Prepare the result lists\n    image_groups = [[] for _ in range(num_groups)]\n    label_groups = [[] for _ in range(num_groups)]\n    \n    # Distribute the data into the groups with repeats allowed\n    for label, items in label_to_data.items():\n        random.shuffle(items)  # Shuffle items within each label group\n        item_cycle = iter(items)\n        for group_index in range(num_groups):\n            for _ in range(k // len(label_to_data)):\n                try:\n                    item = next(item_cycle)\n                except StopIteration:\n                    # If we run out of items for a label, shuffle and start again\n                    random.shuffle(items)\n                    item_cycle = iter(items)\n                    item = next(item_cycle)\n                image_groups[group_index].append(item)\n                label_groups[group_index].append(label)\n\n    return image_groups, label_groups\n\ndef split_data_randomly(data, labels, num_groups, k):\n    \"\"\"\n    Split the data and labels randomly into a specified number of groups with each group containing exactly k samples.\n    Items can be repeated within and across groups to ensure balanced distribution.\n\n    Args:\n    - data: List of data elements\n    - labels: List of corresponding labels\n    - num_groups: Number of groups to split the data into\n    - k: Number of samples in each group\n\n    Returns:\n    - image_groups: List of groups containing data elements\n    - label_groups: List of groups containing corresponding labels\n    \"\"\"\n    # Ensure data and labels have the same length\n    assert len(data) == len(labels), \"Data and labels must have the same length.\"\n    \n    # Shuffle data and labels together\n    combined_data = list(zip(data, labels))\n    random.shuffle(combined_data)\n    \n    # Prepare the result lists\n    image_groups = [[] for _ in range(num_groups)]\n    label_groups = [[] for _ in range(num_groups)]\n    \n    # Fill each group with k samples, allowing repetition\n    for i in range(num_groups):\n        for j in range(k):\n            item = combined_data[random.randint(0, len(combined_data) - 1)]\n            image_groups[i].append(item[0])\n            label_groups[i].append(item[1])\n    \n    return image_groups, label_groups","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport pandas as pd\nimport cv2\nimport os\nimport torch\nfrom PIL import Image\nimport pydicom\n\nfrom torchvision import transforms\nfrom typing import Tuple\n\n# mean = [0.2652, 0.2652, 0.2652]\n# std = [0.1994, 0.1994, 0.1994]\n\ndata_transforms = {\n    'training': transforms.Compose([\n#         transforms.RandomHorizontalFlip(p=0.3),\n#         transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(),]),p=0.3),\n#         transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=3),]),p=0.3),\n        transforms.ToTensor(),\n#         transforms.Normalize(mean, std)\n    ]),\n    'valid': transforms.Compose([\n        transforms.ToTensor(),\n#         transforms.Normalize(mean, std)\n    ]),\n    'test': transforms.Compose([\n        transforms.ToTensor(),\n#         transforms.Normalize(mean, std)\n    ]),\n}\n\nclass SeveritySimilarityDataset(Dataset):\n    def __init__(self, annotation_file_path: str, dataset_dir: str, phase: str = \"training\", num_per_cluster: int = 5, num_group: int = 10000, input_size: Tuple[int] = (224, 224), transforms=None) -> None:\n        \"\"\"\n        Dataset class for severity similarity task.\n\n        Args:\n        - annotation_df: DataFrame containing annotations\n        - dataset_dir: Directory containing image data\n        - phase: Phase of the dataset (e.g., \"training\", \"validation\", \"testing\")\n        - num_per_cluster: Number of images per cluster\n        \"\"\"\n        super(SeveritySimilarityDataset, self).__init__()\n        self.dataset_dir = dataset_dir\n        annotation_df = pd.read_csv(annotation_file_path)\n        # Filter data based on the specified phase\n        data = annotation_df[annotation_df[\"split\"] == phase]\n        # Concatenate study_id and image_id to get image paths\n        image_paths_df = data[\"study_id\"] + \"/\" + data[\"image_id\"] +\".png\"\n        image_paths = image_paths_df.tolist()\n        self.transforms = data_transforms[phase] if transforms == None else transforms\n        # Get labels\n        labels_df = data[\"breast_birads\"]\n        labels = labels_df.to_list()\n\n        # Split data into clusters\n#         if phase == \"training\":\n        self.image_cluster_list, self.label_cluster_list = split_data_balanced_randomly(image_paths, labels, num_group, num_per_cluster)\n#         else:\n#             self.image_cluster_list, self.label_cluster_list = split_data_randomly(image_paths, labels, num_group, num_per_cluster)\n\n\n        self.input_size = input_size\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of clusters in the dataset.\n        \"\"\"\n        return len(self.label_cluster_list)\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Retrieves a cluster of images and its corresponding label cluster.\n\n        Args:\n        - index: Index of the cluster to retrieve\n\n        Returns:\n        - images: List of images in the cluster\n        - gram_matrix: Gram matrix computed from the label cluster\n        \"\"\"\n        image_cluster = self.image_cluster_list[index]\n        label_cluster = self.label_cluster_list[index]\n        images = []\n        for image_path in image_cluster:\n            abs_image_path = os.path.join(self.dataset_dir, image_path)\n            \n            # Read and preprocess image\n            image =  self._read_image(os.path.join(self.dataset_dir,image_path), self.input_size) # Transpose image tensor\n            images.append(image)\n        # # Compute Gram matrix\n        gram_matrix_ = gram_matrix(label_cluster)\n        return  images, torch.tensor(gram_matrix_).to(torch.float)\n    \n    def _read_image(self, filepath, new_size):\n        image_pil = Image.open(filepath)\n        \n        # Kiểm tra chế độ của ảnh\n        if image_pil.mode != 'L':\n            image_pil = image_pil.convert('L')  # Chuyển đổi sang chế độ 'L' (grayscale) nếu cần thiết\n        \n        # Tạo ảnh RGB từ ảnh đơn kênh bằng cách sao chép giá trị của kênh đó vào cả ba kênh\n        image_pil = Image.merge('RGB', (image_pil, image_pil, image_pil))\n        \n    \n        resized_image = self.transforms(image_pil)\n        resized_image = resized_image.to(torch.float)\n        \n        return resized_image\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nimport torch\nimport torchvision.models as models\nimport timm\n\nclass Setting_2_model(nn.Module):\n    def __init__(self, model_name: str, embed_dim: int):\n        \"\"\"\n        A custom model for Setting 2, which uses different pre-trained models\n        based on the specified `model_name`.\n\n        Args:\n        - model_name: Name of the pre-trained model to be used\n        - embed_dim: Dimension of the output embeddings\n        \"\"\"\n        super(Setting_2_model, self).__init__()\n        self.model_name = model_name\n        # Load the specified pre-trained model\n        if model_name.startswith('resnet'):\n            if model_name == 'resnet50':\n                self.model = models.resnet50(weights = models.ResNet50_Weights.IMAGENET1K_V2)\n            elif model_name == 'resnet101':\n                self.model = models.resnet101(weights = models.ResNet101_Weights.IMAGENET1K_V2)\n            elif model_name == 'resnet152':\n                self.model = models.resnet152(weights = models.ResNet152_Weights.IMAGENET1K_V2)\n            else:\n                raise ValueError(f\"Unsupported ResNet model: {model_name}\")\n                \n            num_features = self.model.fc.in_features\n            self.model.fc = nn.Linear(num_features, embed_dim)\n        \n        elif model_name.startswith('densenet'):\n            if model_name == 'densenet121':\n                self.model = models.densenet121(weights = models.DenseNet121_Weights.IMAGENET1K_V1)\n            else:\n                raise ValueError(f\"Unsupported DenseNet model: {model_name}\")\n                \n            num_features = self.model.classifier.in_features\n            self.model.classifier = nn.Linear(num_features, embed_dim)\n        \n        elif model_name.startswith('vit'):\n            self.model = timm.create_model(model_name, pretrained=True)\n\n            num_features = self.model.head.in_features\n            self.model.head = nn.Linear(num_features, embed_dim)\n        \n        else:\n            raise ValueError(f\"Unsupported model: {model_name}\")\n    \n    def forward(self, images, device):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n        - images: A list of input images\n\n        Returns:\n        - gram_matrix: The Gram matrix computed from the embeddings\n        \"\"\"\n        embeddings = []\n        # Iterate over the list of input images\n        for image in images:\n            # Pass the image through the pre-trained model\n            image = image.to(device)\n            image_embedding = self.model(image)\n            # Append the embedding to the list\n            embeddings.append(image_embedding)\n        # Stack the embeddings along a new dimension\n        embeddings_tensor = torch.stack(embeddings, dim=1)\n        # Normalize the embeddings\n        embeddings_normalized = torch.nn.functional.normalize(embeddings_tensor, p=2, dim=2)\n\n        # Compute the Gram matrix\n        gram_matrix = torch.matmul(embeddings_normalized, embeddings_normalized.transpose(1, 2))\n        gram_matrix = torch.ones_like(gram_matrix) - gram_matrix\n        return gram_matrix.to(device)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, margin=2.0):\n        \"\"\"\n        Contrastive Loss function for computing the loss between predicted\n        and ground truth Gram matrices.\n\n        Args:\n        - margin: Margin value for the loss calculation\n        \"\"\"\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, gram_matrix_predicted, gram_matrix_ground_truth):\n        \"\"\"\n        Forward pass of the Contrastive Loss function.\n\n        Args:\n        - gram_matrix_predicted: Predicted Gram matrix\n        - gram_matrix_ground_truth: Ground truth Gram matrix\n\n        Returns:\n        - loss: Contrastive Learning Loss\n        \"\"\"\n       \n        loss = (1-gram_matrix_ground_truth )* torch.pow(gram_matrix_predicted, 2) + (gram_matrix_ground_truth) * torch.pow(torch.clamp(self.margin - gram_matrix_predicted, min=0.0), 2)\n        return torch.mean(loss)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom tqdm.notebook import tqdm\nfrom torch.optim import lr_scheduler\n\ndef train_model(model, train_dataset, val_dataset, checkpoint_folder, num_epochs=10, batch_size=32, learning_rate=0.001):\n    \"\"\"\n    Train the model using the provided datasets.\n\n    Args:\n    - model: The model to be trained\n    - train_dataset: Dataset for training\n    - val_dataset: Dataset for validation\n    - checkpoint_folder: Folder to store checkpoints\n    - num_epochs: Number of epochs for training\n    - batch_size: Batch size for training\n    - learning_rate: Learning rate for optimization\n\n    Returns:\n    - model: Trained model\n    - train_losses: List of training losses\n    - val_losses: List of validation losses\n    \"\"\"\n    # Create the checkpoint folder if it doesn't exist\n    if not os.path.exists(checkpoint_folder):\n        os.makedirs(checkpoint_folder)\n    device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    # Define data loaders for training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # Define loss function and optimizer\n    criterion = ContrastiveLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\n    # Lists to store training and validation losses\n    train_losses = []\n    val_losses = []\n\n    # Variables to keep track of the best model and its performance\n    best_val_loss = float('inf')\n    best_model_state = None\n\n    model = model.to(device)\n    print(\"Training started...\")\n    for epoch in tqdm(range(num_epochs)):\n        torch.cuda.empty_cache()\n        print(\"*\"*100)\n        print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n        model.train()\n        running_train_loss = 0.0\n        for i, (images, labels) in enumerate(train_loader):\n            optimizer.zero_grad()\n            # Forward pass\n            labels = labels.to(device)\n            outputs = model(images, device)\n            # Compute loss\n            loss = criterion(outputs, labels)\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            running_train_loss += loss.item()\n\n            if i % 100 == 0:\n                print(f\"\\t Batch [{i}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n        \n        # Compute average training loss for the epoch\n        epoch_train_loss = running_train_loss / len(train_loader)\n        train_losses.append(epoch_train_loss)\n\n        # Validation loop\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for i, (images, labels) in enumerate(val_loader):\n                labels = labels.to(device)\n                outputs = model(images, device)\n                loss = criterion(outputs, labels)\n                running_val_loss += loss.item()\n\n                if i % 50 == 0:\n                    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Batch [{i}/{len(val_loader)}], Val Loss: {loss.item():.4f}\")\n        \n        # Compute average validation loss for the epoch\n        epoch_val_loss = running_val_loss / len(val_loader)\n        val_losses.append(epoch_val_loss)\n\n        # Save the model checkpoint for every epoch (last model)\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': epoch_val_loss\n        }, os.path.join(checkpoint_folder, 'last.pt'))\n\n        # Save the best model checkpoint based on validation loss\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss\n            best_model_state = model.state_dict()\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': best_model_state,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': best_val_loss\n            }, os.path.join(checkpoint_folder, 'best.pt'))\n\n        # Print progress\n        print(f\"Validation, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n        print(\"*\"*100)\n        scheduler.step()\n    print(\"Training completed.\")\n\n    return model, train_losses, val_losses\n\n\ndef test_model(model, test_dataset, batch_size=32):\n    \"\"\"\n    Evaluate the model on the test dataset.\n\n    Args:\n    - model: The trained model to be evaluated\n    - test_dataset: Dataset for testing\n    - batch_size: Batch size for testing\n\n    Returns:\n    - test_loss: Test loss\n    \"\"\"\n    # Define data loader for testing\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Define loss function\n    criterion = nn.MSELoss()\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize variables for computing test loss\n    running_test_loss = 0.0\n    num_samples = 0\n\n    print(\"Testing started...\")\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_test_loss += loss.item() * images.size(0)\n            num_samples += images.size(0)\n\n    # Compute test loss\n    test_loss = running_test_loss / num_samples\n\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(\"Testing completed.\")\n\n    return test_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"annotation_data_path\": \"/kaggle/input/mammo-224-224-ver2/split_data.csv\",\n    \"image_folder_path\": \"/kaggle/input/mammo-224-224-ver2/Processed_Images\",\n    \"model_encoder\": \"resnet50\",\n    \"embedding_dim\": 512, \n    \"learning_rate\": 1e-2,\n    \"num_epoch\":50,\n    \"batch_size\": 8,\n    \"num_per_cluster\": 5,\n    \"num_groups\": 5000,\n    \"checkpoint\": \"\",\n    \"checkpoint_folder\": \"/kaggle/working/weights_setting2/resnet50BasedModel\"\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = SeveritySimilarityDataset(annotation_file_path=config[\"annotation_data_path\"],\n                                    dataset_dir=config[\"image_folder_path\"],\n                                    phase=\"training\",\n                                    num_per_cluster=config[\"num_per_cluster\"]+10,\n                                    num_group = config[\"num_groups\"],\n                                    input_size=(224, 224)\n                                    )\n\nvalid_dataset = SeveritySimilarityDataset(annotation_file_path=config[\"annotation_data_path\"],\n                                    dataset_dir=config[\"image_folder_path\"],\n                                    phase=\"valid\",\n                                    num_per_cluster=config[\"num_per_cluster\"],\n                                    num_group = config[\"num_groups\"]//10,\n                                    input_size=(224, 224)\n                                    )\nmodel = Setting_2_model(model_name=config[\"model_encoder\"],\n                        embed_dim=config[\"embedding_dim\"]\n                        )\nif config[\"checkpoint\"]:\n    checkpoint = torch.load(config[\"checkpoint\"])\n    print(\"Checkpoint: {}\".format(config[\"checkpoint\"]))\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n\ntrain_model(model=model, train_dataset=train_dataset,\n            val_dataset=valid_dataset, num_epochs=config[\"num_epoch\"],\n            batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n            checkpoint_folder=config[\"checkpoint_folder\"]\n            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}