{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_comparison(a, b):\n",
    "    \"\"\"\n",
    "    This function compares two values a and b.\n",
    "    If they are greater, it returns 1.\n",
    "    If they are less, it return 0\n",
    "    If they are not equal, it returns 2.\n",
    "    \"\"\"\n",
    "    return ((a>b))*1 if (a!=b) else 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def gram_matrix(list_of_score):\n",
    "    \"\"\"\n",
    "    This function computes the Gram matrix for a list of scores.\n",
    "    \n",
    "    The Gram matrix is a matrix of pairwise comparisons of scores.\n",
    "    Each element [i][j] in the matrix represents the result of\n",
    "    comparing list_of_score[i] with list_of_score[j] using the\n",
    "    pair_comparison function.\n",
    "    \n",
    "    Args:\n",
    "    - list_of_score: A list of scores\n",
    "    \n",
    "    Returns:\n",
    "    - gram_matrix: The Gram matrix computed from the pairwise comparisons\n",
    "    \"\"\"\n",
    "    # Get the length of the list of scores\n",
    "    n = len(list_of_score)\n",
    "    \n",
    "    # Initialize the Gram matrix with zeros\n",
    "    gram_matrix = [[0 for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    # Iterate through each pair of scores\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Compute the pairwise comparison using the pair_comparison function\n",
    "            gram_matrix[i][j] = pair_comparison(list_of_score[i], list_of_score[j])\n",
    "    \n",
    "    # Return the computed Gram matrix\n",
    "    return gram_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5, 0, 0, 0, 0, 0, 0, 0.5, 0],\n",
       " [1, 0.5, 0, 0.5, 0, 0, 0.5, 1, 0],\n",
       " [1, 1, 0.5, 1, 0.5, 0, 1, 1, 0],\n",
       " [1, 0.5, 0, 0.5, 0, 0, 0.5, 1, 0],\n",
       " [1, 1, 0.5, 1, 0.5, 0, 1, 1, 0],\n",
       " [1, 1, 1, 1, 1, 0.5, 1, 1, 0.5],\n",
       " [1, 0.5, 0, 0.5, 0, 0, 0.5, 1, 0],\n",
       " [0.5, 0, 0, 0, 0, 0, 0, 0.5, 0],\n",
       " [1, 1, 1, 1, 1, 0.5, 1, 1, 0.5]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "list_of_score = [1,2,3,2,3,4,2,1,4]\n",
    "gram_matrix(list_of_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_randomly(data, labels, k):\n",
    "    \"\"\"\n",
    "    Split the data and labels randomly into groups of size k.\n",
    "\n",
    "    Args:\n",
    "    - data: List of data elements\n",
    "    - labels: List of corresponding labels\n",
    "    - k: Size of each group\n",
    "\n",
    "    Returns:\n",
    "    - image_groups: List of groups containing data elements\n",
    "    - label_groups: List of groups containing corresponding labels\n",
    "    \"\"\"\n",
    "    # Shuffle data and labels in sync\n",
    "    combined_data = list(zip(data, labels))\n",
    "    random.shuffle(combined_data)\n",
    "    # Split the shuffled data into groups of size k\n",
    "    num_groups = len(combined_data) // k\n",
    "    image_groups = [data[i * k : (i + 1) * k] for i in range(num_groups)]\n",
    "    label_groups = [labels[i * k : (i + 1) * k] for i in range(num_groups)]\n",
    "    return image_groups, label_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import random\n",
    "\n",
    "from torchvision import transforms\n",
    "from typing import Tuple\n",
    "class SeveritySimilarityDataset(Dataset):\n",
    "    def __init__(self, annotation_file_path: str, dataset_dir: str, phase: str = \"training\", num_per_cluster: int = 5, input_size: Tuple[int] = (224, 224)) -> None:\n",
    "        \"\"\"\n",
    "        Dataset class for severity similarity task.\n",
    "\n",
    "        Args:\n",
    "        - annotation_df: DataFrame containing annotations\n",
    "        - dataset_dir: Directory containing image data\n",
    "        - phase: Phase of the dataset (e.g., \"training\", \"validation\", \"testing\")\n",
    "        - num_per_cluster: Number of images per cluster\n",
    "        \"\"\"\n",
    "        super(SeveritySimilarityDataset, self).__init__()\n",
    "        self.dataset_dir = dataset_dir\n",
    "        annotation_df = pd.read_csv(annotation_file_path)\n",
    "        # Filter data based on the specified phase\n",
    "        data = annotation_df[annotation_df[\"split\"] == phase]\n",
    "        # Concatenate study_id and image_id to get image paths\n",
    "        image_paths_df = data[\"study_id\"] + \"/\" + data[\"image_id\"] +\".png\"\n",
    "        self.image_paths = image_paths_df.tolist()\n",
    "        self.num_per_cluster = num_per_cluster\n",
    "        # Get labels\n",
    "        labels_df = data[\"breast_birads\"]\n",
    "        self.labels = [int(s[-1]) for s in labels_df.to_list()]\n",
    "        \n",
    "        # Split data into clusters\n",
    "        self.image_cluster_list, self.label_cluster_list = split_data_randomly(self.image_paths, self.labels, self.num_per_cluster)\n",
    "\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of clusters in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.label_cluster_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves a cluster of images and its corresponding label cluster.\n",
    "\n",
    "        Args:\n",
    "        - index: Index of the cluster to retrieve\n",
    "\n",
    "        Returns:\n",
    "        - images: List of images in the cluster\n",
    "        - gram_matrix: Gram matrix computed from the label cluster\n",
    "        \"\"\"\n",
    "        image_cluster = self.image_cluster_list[index]\n",
    "        label_cluster = self.label_cluster_list[index]\n",
    "        images = []\n",
    "        for image_path in image_cluster:\n",
    "            abs_image_path = os.path.join(self.dataset_dir, image_path)\n",
    "            # Read and preprocess image\n",
    "            image =  self._read_image(os.path.join(self.dataset_dir,image_path), self.input_size) # Transpose image tensor\n",
    "            images.append(image)\n",
    "        # # Compute Gram matrix\n",
    "        gram_matrix_ = gram_matrix(label_cluster)\n",
    "        ref_images = self.get_ref()\n",
    "        return  images, ref_images, torch.tensor(gram_matrix_).to(torch.float), torch.tensor(label_cluster).to(torch.float)\n",
    "    \n",
    "    def get_ref(self):\n",
    "        ref_idxs = [i for i in range(len(self.labels)) if self.labels[i] == 1]\n",
    "        random_ref_id = random.choice(ref_idxs)\n",
    "        ref_image_path = self.image_paths[random_ref_id]\n",
    "        ref_image =  self._read_image(os.path.join(self.dataset_dir,ref_image_path), self.input_size)\n",
    "        ref_images = []\n",
    "        for i in range (self.num_per_cluster):\n",
    "            ref_images.append(ref_image)\n",
    "        return ref_images\n",
    "    \n",
    "    def _read_image(self, filepath, new_size):\n",
    "        image_pil = Image.open(filepath)\n",
    "        \n",
    "        # Kiểm tra chế độ của ảnh\n",
    "        if image_pil.mode != 'L':\n",
    "            image_pil = image_pil.convert('L')  # Chuyển đổi sang chế độ 'L' (grayscale) nếu cần thiết\n",
    "        \n",
    "        # Tạo ảnh RGB từ ảnh đơn kênh bằng cách sao chép giá trị của kênh đó vào cả ba kênh\n",
    "        image_pil = Image.merge('RGB', (image_pil, image_pil, image_pil))\n",
    "        \n",
    "        # Resize ảnh\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(new_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        resized_image = transform(image_pil)\n",
    "        resized_image = resized_image.to(torch.float)\n",
    "        \n",
    "        return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2133\n",
      "([tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.4863, 0.4431, 0.4549],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.4157, 0.4118, 0.3922],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3882, 0.3765, 0.3725],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1686, 0.1608, 0.1412],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.1882, 0.1882],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1373, 0.1137, 0.1176]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.4863, 0.4431, 0.4549],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.4157, 0.4118, 0.3922],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3882, 0.3765, 0.3725],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1686, 0.1608, 0.1412],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.1882, 0.1882],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1373, 0.1137, 0.1176]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.4863, 0.4431, 0.4549],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.4157, 0.4118, 0.3922],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3882, 0.3765, 0.3725],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1686, 0.1608, 0.1412],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.1882, 0.1882],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1373, 0.1137, 0.1176]]]), tensor([[[0.1961, 0.1608, 0.1059,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6078, 0.5529, 0.4745,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6235, 0.6431, 0.6431,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1098, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0745, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0510, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1961, 0.1608, 0.1059,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6078, 0.5529, 0.4745,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6235, 0.6431, 0.6431,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1098, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0745, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0510, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1961, 0.1608, 0.1059,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6078, 0.5529, 0.4745,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6235, 0.6431, 0.6431,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1098, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0745, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0510, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]), tensor([[[0.6471, 0.6510, 0.6980,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6235, 0.6157, 0.6196,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6510, 0.6510, 0.6039,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1569, 0.1647, 0.1882,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2353, 0.2157, 0.1686,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1804, 0.1412, 0.1569,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6471, 0.6510, 0.6980,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6235, 0.6157, 0.6196,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6510, 0.6510, 0.6039,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1569, 0.1647, 0.1882,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2353, 0.2157, 0.1686,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1804, 0.1412, 0.1569,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6471, 0.6510, 0.6980,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6235, 0.6157, 0.6196,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6510, 0.6510, 0.6039,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1569, 0.1647, 0.1882,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2353, 0.2157, 0.1686,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1804, 0.1412, 0.1569,  ..., 0.0000, 0.0000, 0.0000]]]), tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.1412, 0.1922],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1686, 0.1451, 0.1451],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.1294, 0.1373],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1804, 0.1843, 0.1843],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2157, 0.2118, 0.2039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.1647, 0.2039]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.1412, 0.1922],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1686, 0.1451, 0.1451],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.1294, 0.1373],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1804, 0.1843, 0.1843],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2157, 0.2118, 0.2039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.1647, 0.2039]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.1412, 0.1922],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1686, 0.1451, 0.1451],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.1294, 0.1373],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1804, 0.1843, 0.1843],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2157, 0.2118, 0.2039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.1647, 0.2039]]]), tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.5373, 0.5412, 0.5255],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5020, 0.5020, 0.5176],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5882, 0.5020, 0.5255],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.6471, 0.5843, 0.6039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5373, 0.6196, 0.6078],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2745, 0.2471, 0.3020]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.5373, 0.5412, 0.5255],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5020, 0.5020, 0.5176],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5882, 0.5020, 0.5255],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.6471, 0.5843, 0.6039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5373, 0.6196, 0.6078],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2745, 0.2471, 0.3020]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.5373, 0.5412, 0.5255],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5020, 0.5020, 0.5176],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5882, 0.5020, 0.5255],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.6471, 0.5843, 0.6039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5373, 0.6196, 0.6078],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2745, 0.2471, 0.3020]]]), tensor([[[0.1255, 0.0510, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1098, 0.0824, 0.1098,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0784, 0.0627, 0.0392,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.7569, 0.7529, 0.7922,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7647, 0.6706, 0.3804,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1412, 0.1804, 0.1922,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1255, 0.0510, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1098, 0.0824, 0.1098,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0784, 0.0627, 0.0392,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.7569, 0.7529, 0.7922,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7647, 0.6706, 0.3804,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1412, 0.1804, 0.1922,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1255, 0.0510, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1098, 0.0824, 0.1098,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0784, 0.0627, 0.0392,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.7569, 0.7529, 0.7922,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7647, 0.6706, 0.3804,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1412, 0.1804, 0.1922,  ..., 0.0000, 0.0000, 0.0000]]])], [tensor([[[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]), tensor([[[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]), tensor([[[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]), tensor([[[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]), tensor([[[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]), tensor([[[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3412, 0.3686, 0.3451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2235, 0.2039, 0.2627,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.8118, 0.7490, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8000, 0.4784, 0.0784,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2706, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]])], tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]]), tensor([1., 1., 1., 1., 1., 1.]))\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "data = SeveritySimilarityDataset(\"split_data.csv\", \"/media/jackson/Data/archive/Processed_Images\", \"training\", 6)\n",
    "print(len(data))\n",
    "print(data[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "class Setting_3_model(nn.Module):\n",
    "    def __init__(self, model_name: str, embed_dim: int):\n",
    "        \"\"\"\n",
    "        A custom model for Setting 2, which uses different pre-trained models\n",
    "        based on the specified `model_name`.\n",
    "\n",
    "        Args:\n",
    "        - model_name: Name of the pre-trained model to be used\n",
    "        - embed_dim: Dimension of the output embeddings\n",
    "        \"\"\"\n",
    "        super(Setting_3_model, self).__init__()\n",
    "\n",
    "        # Load the specified pre-trained model\n",
    "        if model_name.startswith('resnet'):\n",
    "            if model_name == 'resnet50':\n",
    "                self.model = models.resnet50(pretrained=True)\n",
    "            elif model_name == 'resnet101':\n",
    "                self.model = models.resnet101(pretrained=True)\n",
    "            elif model_name == 'resnet152':\n",
    "                self.model = models.resnet152(pretrained=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ResNet model: {model_name}\")\n",
    "                \n",
    "            num_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(num_features, embed_dim)\n",
    "        \n",
    "        elif model_name.startswith('densenet'):\n",
    "            if model_name == 'densenet121':\n",
    "                self.model = models.densenet121(pretrained=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported DenseNet model: {model_name}\")\n",
    "                \n",
    "            num_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(num_features, embed_dim)\n",
    "        \n",
    "        elif model_name.startswith('vit'):\n",
    "            self.model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "            num_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(num_features, embed_dim)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    def forward(self, images, ref_images):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "        - images: A list of input images\n",
    "\n",
    "        Returns:\n",
    "        - gram_matrix: The Gram matrix computed from the embeddings\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        ref_embedding = []\n",
    "        # Iterate over the list of input images\n",
    "        for image, ref_image in zip(images, ref_images):\n",
    "            # Pass the image through the pre-trained model\n",
    "            image_embedding = self.model(image)\n",
    "            ref_image_embedding = self.model(ref_image)\n",
    "            # Append the embedding to the list\n",
    "            embeddings.append(image_embedding)\n",
    "            ref_embedding.append(ref_image_embedding)\n",
    "        # Stack the embeddings along a new dimension\n",
    "        embeddings_tensor = torch.stack(embeddings, dim=1)\n",
    "        ref_embedding_tensor = torch.stack(ref_embedding, dim=1)\n",
    "        \n",
    "        return embeddings_tensor, ref_embedding_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PreferenceComparisonLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        \"\"\"\n",
    "        Preference Comparison Loss function for computing the loss between predicted\n",
    "        and ground truth Gram matrices.\n",
    "\n",
    "        Args:\n",
    "        - margin: Margin value for the loss calculation\n",
    "        \"\"\"\n",
    "        super(PreferenceComparisonLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output, label, ref):\n",
    "        \"\"\"\n",
    "        Forward pass of the Preference Comparison Loss function.\n",
    "\n",
    "        Args:\n",
    "        - output: Predicted matrix from the model, shape (batch_size, n, embedding_dim)\n",
    "        - ref: Reference matrix (ground truth), shape (batch_size, n, embedding_dim)\n",
    "        - label: Ground truth labels, shape (batch_size, n, n)\n",
    "\n",
    "        Returns:\n",
    "        - loss: Preference Comparison Loss\n",
    "        \"\"\"\n",
    "        # Calculate cosine similarity between output and ref matrices\n",
    "        cosine_similarities = F.cosine_similarity(output, ref, dim=2)\n",
    "        # Initialize loss variable\n",
    "        loss_contrastive = 0\n",
    "        count_pairs = 0\n",
    "\n",
    "        # Loop over each pair of images in the batch\n",
    "        for i in range(output.size(0)):\n",
    "            for j in range(output.size(1)):\n",
    "                for k in range(output.size(1)):\n",
    "                    if j != k:\n",
    "                        # Get cosine similarity distances\n",
    "                        cosine_distanceA = cosine_similarities[i, j]\n",
    "                        cosine_distanceB = cosine_similarities[i, k]\n",
    "\n",
    "                        # Get label for the pair (j, k)\n",
    "                        label_value = label[i, j, k].item()\n",
    "                        # print(cosine_distanceA, type(cosine_distanceA), cosine_distanceB, type(cosine_distanceB), label_value, type(label_value))\n",
    "                        # Calculate loss based on label\n",
    "                        # if label_value < 2:\n",
    "                        #     loss_contrastive += torch.pow(torch.clamp(torch.abs(cosine_distanceA - cosine_distanceB) + self.margin, min=0.0), 2)\n",
    "                        #     count_pairs += 1\n",
    "                        # else:\n",
    "                        #     # Same difference\n",
    "                        #     loss_contrastive += torch.pow(torch.clamp(self.margin - torch.abs(cosine_distanceA - cosine_distanceB), min=0.0), 2)\n",
    "                        #     count_pairs += 1\n",
    "                        loss_contrastive += torch.nn.BCEWithLogitsLoss()(torch.nn.Sigmoid()(cosine_distanceA - cosine_distanceB), torch.tensor(label_value, dtype=torch.float32))\n",
    "                        \n",
    "                        count_pairs += 1\n",
    "        # Check if there are valid pairs to compute loss\n",
    "        if count_pairs > 0:\n",
    "            # Average loss across the valid pairs\n",
    "            loss_contrastive /= count_pairs\n",
    "        else:\n",
    "            # If there are no valid pairs, return zero loss\n",
    "            loss_contrastive = torch.tensor(0.0, requires_grad=True, device=output.device)\n",
    "\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# import torch\n",
    "\n",
    "# gt = [1,2,3,2,3,4,2,2,4]\n",
    "# gt = torch.tensor(gram_matrix(gt),dtype=torch.float)\n",
    "# pred = [1,2,3,2,3,4,2,1,4]\n",
    "# pred = torch.tensor(gram_matrix(pred), dtype=torch.float)\n",
    "# criterion = ContrastiveLoss()\n",
    "# loss = criterion(pred, gt)\n",
    "\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, num_epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the model using the provided datasets.\n",
    "\n",
    "    Args:\n",
    "    - model: The model to be trained\n",
    "    - train_dataset: Dataset for training\n",
    "    - val_dataset: Dataset for validation\n",
    "    - num_epochs: Number of epochs for training\n",
    "    - batch_size: Batch size for training\n",
    "    - learning_rate: Learning rate for optimization\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained model\n",
    "    - train_losses: List of training losses\n",
    "    - val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    # Define data loaders for training and validation\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = PreferenceComparisonLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Lists to store training and validation losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(\"Training started...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for i, (images, ref_images, gram_matrix, labels) in enumerate(train_loader, 1):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output, ref_output = model(images, ref_images)\n",
    "            # Compute loss\n",
    "            loss = criterion(output, gram_matrix,ref_output)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Compute average training loss for the epoch\n",
    "        epoch_train_loss = running_train_loss / len(train_dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(val_loader, 1):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Batch [{i}/{len(val_loader)}], Val Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Compute average validation loss for the epoch\n",
    "        epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "def test_model(model, test_dataset, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained model to be evaluated\n",
    "    - test_dataset: Dataset for testing\n",
    "    - batch_size: Batch size for testing\n",
    "\n",
    "    Returns:\n",
    "    - test_loss: Test loss\n",
    "    \"\"\"\n",
    "    # Define data loader for testing\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables for computing test loss\n",
    "    running_test_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    print(\"Testing started...\")\n",
    "    with torch.no_grad():\n",
    "        for images, ref_images, gram_matrix, labels in test_loader:\n",
    "            outputs = model(images, ref_images)\n",
    "            loss = criterion(outputs, gram_matrix, ref_images)\n",
    "            running_test_loss += loss.item() * images.size(0)\n",
    "            num_samples += images.size(0)\n",
    "\n",
    "    # Compute test loss\n",
    "    test_loss = running_test_loss / num_samples\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(\"Testing completed.\")\n",
    "\n",
    "    return test_loss\n",
    "def _read_resize_dicom(self, filepath, new_size):\n",
    "         # Đọc file DICOM\n",
    "        dicom_data = pydicom.dcmread(filepath)\n",
    "        \n",
    "        # Chuyển đổi dữ liệu DICOM thành mảng numpy\n",
    "        image_array = dicom_data.pixel_array\n",
    "        \n",
    "        # Chuyển đổi mảng numpy thành ảnh PIL\n",
    "        image_pil = Image.fromarray(image_array)\n",
    "        \n",
    "        # Kiểm tra chế độ của ảnh\n",
    "        if image_pil.mode != 'L':\n",
    "            image_pil = image_pil.convert('L')  # Chuyển đổi sang chế độ 'L' (grayscale) nếu cần thiết\n",
    "        \n",
    "        # Tạo ảnh RGB từ ảnh đơn kênh bằng cách sao chép giá trị của kênh đó vào cả ba kênh\n",
    "        image_pil = Image.merge('RGB', (image_pil, image_pil, image_pil))\n",
    "        \n",
    "        # Resize ảnh\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(new_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        resized_image = transform(image_pil)\n",
    "        resized_image = resized_image.to(torch.float)\n",
    "        \n",
    "        return resized_image\n",
    "\n",
    "# Example usage:\n",
    "# model = Setting_2_model(model_name='resnet50', embed_dim=512)\n",
    "# train_dataset = SeveritySimilarityDataset(train_annotation_df, dataset_dir, phase='training')\n",
    "# val_dataset = SeveritySimilarityDataset(val_annotation_df, dataset_dir, phase='validation')\n",
    "# test_dataset = SeveritySimilarityDataset(test_annotation_df, dataset_dir, phase='testing')\n",
    "# trained_model, train_losses, val_losses = train_model(model, train_dataset, val_dataset, num_epochs=10, batch_size=32, learning_rate=0.001)\n",
    "# test_loss = test_model(trained_model, test_dataset, batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"annotation_data_path\": \"split_data.csv\",\n",
    "    \"image_folder_path\": \"/media/jackson/Data/archive/Processed_Images\",\n",
    "    \"model_encoder\": \"resnet50\",\n",
    "    \"embedding_dim\": 512, \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_epoch\": 5,\n",
    "    \"batch_size\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackson/anaconda3/envs/Paper/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jackson/anaconda3/envs/Paper/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch [1/5], Batch [10/640], Train Loss: 0.7236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optim \u001b[38;5;28;01mas\u001b[39;00m opt\n\u001b[1;32m     22\u001b[0m optimzer \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 47\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, val_dataset, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, gram_matrix,ref_output)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/Paper/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Paper/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = SeveritySimilarityDataset(annotation_file_path=config[\"annotation_data_path\"],\n",
    "                                    dataset_dir=config[\"image_folder_path\"],\n",
    "                                    phase=\"training\",\n",
    "                                    num_per_cluster=5,\n",
    "                                    input_size=(224, 224)\n",
    "                                    )\n",
    "\n",
    "test_dataset = SeveritySimilarityDataset(annotation_file_path=config[\"annotation_data_path\"],\n",
    "                                    dataset_dir=config[\"image_folder_path\"],\n",
    "                                    phase=\"valid\",\n",
    "                                    num_per_cluster=5,\n",
    "                                    input_size=(224, 224)\n",
    "                                    )\n",
    "model = Setting_3_model(model_name=config[\"model_encoder\"],\n",
    "                        embed_dim=config[\"embedding_dim\"]\n",
    "                        )\n",
    "\n",
    "criterion = PreferenceComparisonLoss()\n",
    "\n",
    "from torch import optim as opt\n",
    "\n",
    "optimzer = opt.SGD(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "\n",
    "train_model(model=model, train_dataset=train_dataset,\n",
    "            val_dataset=test_dataset, num_epochs=config[\"num_epoch\"],\n",
    "            batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
