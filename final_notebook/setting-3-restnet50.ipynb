{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-31T16:35:03.855622Z","iopub.status.busy":"2024-05-31T16:35:03.855057Z","iopub.status.idle":"2024-05-31T16:35:03.862496Z","shell.execute_reply":"2024-05-31T16:35:03.861007Z","shell.execute_reply.started":"2024-05-31T16:35:03.855584Z"},"trusted":true},"outputs":[],"source":["def pair_comparison(a, b):\n","    \"\"\"\n","    This function compares two values a and b.\n","    If they are greater, it returns 1.\n","    If they are less, it return 0\n","    If they are not equal, it returns 2.\n","    \"\"\"\n","    return (a>b)* 1 if a!=b else 2\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T16:35:03.865164Z","iopub.status.busy":"2024-05-31T16:35:03.864751Z","iopub.status.idle":"2024-05-31T16:35:03.878901Z","shell.execute_reply":"2024-05-31T16:35:03.877507Z","shell.execute_reply.started":"2024-05-31T16:35:03.865126Z"},"trusted":true},"outputs":[],"source":["import random\n","def gram_matrix(list_of_score):\n","    \"\"\"\n","    This function computes the Gram matrix for a list of scores.\n","    \n","    The Gram matrix is a matrix of pairwise comparisons of scores.\n","    Each element [i][j] in the matrix represents the result of\n","    comparing list_of_score[i] with list_of_score[j] using the\n","    pair_comparison function.\n","    \n","    Args:\n","    - list_of_score: A list of scores\n","    \n","    Returns:\n","    - gram_matrix: The Gram matrix computed from the pairwise comparisons\n","    \"\"\"\n","    # Get the length of the list of scores\n","    n = len(list_of_score)\n","    \n","    # Initialize the Gram matrix with zeros\n","    gram_matrix = [[0 for _ in range(n)] for _ in range(n)]\n","\n","    # Iterate through each pair of scores\n","    for i in range(n):\n","        for j in range(n):\n","            # Compute the pairwise comparison using the pair_comparison function\n","            gram_matrix[i][j] = pair_comparison(list_of_score[i], list_of_score[j])\n","    \n","    # Return the computed Gram matrix\n","    return gram_matrix\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T16:35:03.880970Z","iopub.status.busy":"2024-05-31T16:35:03.880610Z","iopub.status.idle":"2024-05-31T16:35:03.899993Z","shell.execute_reply":"2024-05-31T16:35:03.898748Z","shell.execute_reply.started":"2024-05-31T16:35:03.880938Z"},"trusted":true},"outputs":[],"source":["import random\n","from collections import defaultdict\n","from itertools import cycle, islice\n","\n","def split_data_balanced_randomly(data, labels, num_groups, k):\n","    \"\"\"\n","    Split the data and labels randomly into a specified number of groups with balanced label distribution.\n","    Each group will contain exactly k samples, with items possibly repeated to ensure balanced distribution.\n","\n","    Args:\n","    - data: List of data elements\n","    - labels: List of corresponding labels\n","    - num_groups: Number of groups to split the data into\n","    - k: Number of samples in each group\n","\n","    Returns:\n","    - image_groups: List of groups containing data elements\n","    - label_groups: List of groups containing corresponding labels\n","    \"\"\"\n","    # Ensure data and labels have the same length\n","    assert len(data) == len(labels), \"Data and labels must have the same length.\"\n","    \n","    # Group data by labels\n","    label_to_data = defaultdict(list)\n","    for item, label in zip(data, labels):\n","        label_to_data[label].append(item)\n","    \n","    # Prepare the result lists\n","    image_groups = [[] for _ in range(num_groups)]\n","    label_groups = [[] for _ in range(num_groups)]\n","    \n","    # Distribute the data into the groups with repeats allowed\n","    for label, items in label_to_data.items():\n","        random.shuffle(items)  # Shuffle items within each label group\n","        item_cycle = iter(items)\n","        for group_index in range(num_groups):\n","            for _ in range(k // len(label_to_data)):\n","                try:\n","                    item = next(item_cycle)\n","                except StopIteration:\n","                    # If we run out of items for a label, shuffle and start again\n","                    random.shuffle(items)\n","                    item_cycle = iter(items)\n","                    item = next(item_cycle)\n","                image_groups[group_index].append(item)\n","                label_groups[group_index].append(label)\n","\n","    return image_groups, label_groups\n","\n","def split_data_randomly(data, labels, num_groups, k):\n","    \"\"\"\n","    Split the data and labels randomly into a specified number of groups with each group containing exactly k samples.\n","    Items can be repeated within and across groups to ensure balanced distribution.\n","\n","    Args:\n","    - data: List of data elements\n","    - labels: List of corresponding labels\n","    - num_groups: Number of groups to split the data into\n","    - k: Number of samples in each group\n","\n","    Returns:\n","    - image_groups: List of groups containing data elements\n","    - label_groups: List of groups containing corresponding labels\n","    \"\"\"\n","    # Ensure data and labels have the same length\n","    assert len(data) == len(labels), \"Data and labels must have the same length.\"\n","    \n","    # Shuffle data and labels together\n","    combined_data = list(zip(data, labels))\n","    random.shuffle(combined_data)\n","    \n","    # Prepare the result lists\n","    image_groups = [[] for _ in range(num_groups)]\n","    label_groups = [[] for _ in range(num_groups)]\n","    \n","    # Fill each group with k samples, allowing repetition\n","    for i in range(num_groups):\n","        for j in range(k):\n","            item = combined_data[random.randint(0, len(combined_data) - 1)]\n","            image_groups[i].append(item[0])\n","            label_groups[i].append(item[1])\n","    \n","    return image_groups, label_groups"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T16:35:03.975045Z","iopub.status.busy":"2024-05-31T16:35:03.974539Z","iopub.status.idle":"2024-05-31T16:35:04.005155Z","shell.execute_reply":"2024-05-31T16:35:04.003170Z","shell.execute_reply.started":"2024-05-31T16:35:03.975012Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","import pandas as pd\n","import cv2\n","import os\n","import torch\n","from PIL import Image\n","import pydicom\n","import random\n","\n","from torchvision import transforms\n","from typing import Tuple\n","\n","mean = [0.6821, 0.4575, 0.2626]\n","std  = [0.1324, 0.1306, 0.1022]\n","data_transforms = {\n","    'training': transforms.Compose([\n","        transforms.Resize((450,200)),\n","        transforms.RandomHorizontalFlip(p=0.3),\n","        transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(),]),p=0.3),\n","        transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=3),]),p=0.3),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize((450,200)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize((450,200)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","}\n","\n","class SeveritySimilarityDataset(Dataset):\n","    def __init__(self, annotation_file_path: str, dataset_dir: str, phase: str = \"training\", num_per_cluster: int = 5, num_group: int = 10000, input_size: Tuple[int] = (224, 224), transforms=None) -> None:\n","        \"\"\"\n","        Dataset class for severity similarity task.\n","\n","        Args:\n","        - annotation_df: DataFrame containing annotations\n","        - dataset_dir: Directory containing image data\n","        - phase: Phase of the dataset (e.g., \"training\", \"validation\", \"testing\")\n","        - num_per_cluster: Number of images per cluster\n","        \"\"\"\n","        super(SeveritySimilarityDataset, self).__init__()\n","        self.dataset_dir = dataset_dir\n","        annotation_df = pd.read_csv(annotation_file_path)\n","        # Filter data based on the specified phase\n","        data = annotation_df[annotation_df[\"split\"] == phase]\n","        self.transforms = data_transforms[phase] if transforms == None else transforms\n","\n","        # Concatenate study_id and image_id to get image paths\n","        image_paths_df = data[\"study_id\"] + \"/\" + data[\"image_id\"] +\".png\"\n","        self.image_paths = image_paths_df.tolist()\n","        self.num_per_cluster = num_per_cluster\n","        # Get labels\n","        labels_df = data[\"breast_birads\"]\n","        self.labels = [int(s[-1]) for s in labels_df.to_list()]\n","        \n","        # Split data into clusters\n","        if phase != \"test\":\n","            self.image_cluster_list, self.label_cluster_list = split_data_balanced_randomly(self.image_paths, self.labels, num_group, self.num_per_cluster)\n","        else:\n","            self.image_cluster_list, self.label_cluster_list = split_data_randomly(self.image_paths, self.labels, num_group, self.num_per_cluster)\n","\n","        self.input_size = input_size\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the number of clusters in the dataset.\n","        \"\"\"\n","        return len(self.label_cluster_list)\n","    \n","    def __getitem__(self, index):\n","        \"\"\"\n","        Retrieves a cluster of images and its corresponding label cluster.\n","\n","        Args:\n","        - index: Index of the cluster to retrieve\n","\n","        Returns:\n","        - images: List of images in the cluster\n","        - gram_matrix: Gram matrix computed from the label cluster\n","        \"\"\"\n","        image_cluster = self.image_cluster_list[index]\n","        label_cluster = self.label_cluster_list[index]\n","        images = []\n","        for image_path in image_cluster:\n","            abs_image_path = os.path.join(self.dataset_dir, image_path)\n","            # Read and preprocess image\n","            image =  self._read_image(os.path.join(self.dataset_dir,image_path), self.input_size) # Transpose image tensor\n","            images.append(image)\n","        # # Compute Gram matrix\n","        gram_matrix_ = gram_matrix(label_cluster)\n","        ref_images = self.get_ref()\n","        return  images, ref_images, torch.tensor(gram_matrix_).to(torch.float), torch.tensor(label_cluster).to(torch.float)\n","    \n","    def get_ref(self):\n","        ref_idxs = [i for i in range(len(self.labels)) if self.labels[i] == 1]\n","        random_ref_id = random.choice(ref_idxs)\n","        ref_image_path = self.image_paths[random_ref_id]\n","        ref_image =  self._read_image(os.path.join(self.dataset_dir,ref_image_path), self.input_size)\n","        ref_images = []\n","        for i in range (self.num_per_cluster):\n","            ref_images.append(ref_image)\n","        return ref_images\n","    \n","    def _read_image(self, filepath, new_size):\n","        image_pil = Image.open(filepath)\n","        \n","        # Kiểm tra chế độ của ảnh\n","        if image_pil.mode != 'L':\n","            image_pil = image_pil.convert('L')  # Chuyển đổi sang chế độ 'L' (grayscale) nếu cần thiết\n","        \n","        # Tạo ảnh RGB từ ảnh đơn kênh bằng cách sao chép giá trị của kênh đó vào cả ba kênh\n","        image_pil = Image.merge('RGB', (image_pil, image_pil, image_pil))\n","        \n","        # Resize ảnh\n","        \n","        resized_image = self.transforms(image_pil)\n","        resized_image = resized_image.to(torch.float)\n","        \n","        return resized_image\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T16:35:04.009367Z","iopub.status.busy":"2024-05-31T16:35:04.008819Z","iopub.status.idle":"2024-05-31T16:35:04.027934Z","shell.execute_reply":"2024-05-31T16:35:04.026772Z","shell.execute_reply.started":"2024-05-31T16:35:04.009317Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","import torch\n","import torchvision.models as models\n","import timm\n","\n","class Setting_3_model(nn.Module):\n","    def __init__(self, model_name: str, embed_dim: int):\n","        \"\"\"\n","        A custom model for Setting 2, which uses different pre-trained models\n","        based on the specified `model_name`.\n","\n","        Args:\n","        - model_name: Name of the pre-trained model to be used\n","        - embed_dim: Dimension of the output embeddings\n","        \"\"\"\n","        super(Setting_3_model, self).__init__()\n","\n","        # Load the specified pre-trained model\n","        if model_name.startswith('resnet'):\n","            if model_name == 'resnet50':\n","                self.model = models.resnet50(pretrained=True)\n","            elif model_name == 'resnet101':\n","                self.model = models.resnet101(pretrained=True)\n","            elif model_name == 'resnet152':\n","                self.model = models.resnet152(pretrained=True)\n","            else:\n","                raise ValueError(f\"Unsupported ResNet model: {model_name}\")\n","                \n","            num_features = self.model.fc.in_features\n","            self.model.fc = nn.Linear(num_features, embed_dim)\n","        \n","        elif model_name.startswith('densenet'):\n","            if model_name == 'densenet121':\n","                self.model = models.densenet121(pretrained=True)\n","            else:\n","                raise ValueError(f\"Unsupported DenseNet model: {model_name}\")\n","                \n","            num_features = self.model.classifier.in_features\n","            self.model.classifier = nn.Linear(num_features, embed_dim)\n","        \n","        elif model_name.startswith('vit'):\n","            self.model = timm.create_model(model_name, pretrained=True)\n","\n","            num_features = self.model.head.in_features\n","            self.model.head = nn.Linear(num_features, embed_dim)\n","        \n","        else:\n","            raise ValueError(f\"Unsupported model: {model_name}\")\n","    \n","    def forward(self, images, ref_images, device):\n","        \"\"\"\n","        Forward pass of the model.\n","\n","        Args:\n","        - images: A list of input images\n","\n","        Returns:\n","        - gram_matrix: The Gram matrix computed from the embeddings\n","        \"\"\"\n","        embeddings = []\n","        ref_embedding = []\n","        # Iterate over the list of input images\n","        for image, ref_image in zip(images, ref_images):\n","            # Pass the image through the pre-trained model\n","            image = image.to(device)\n","            image_embedding = self.model(image)\n","            ref_image = ref_image.to(device)\n","            ref_image_embedding = self.model(ref_image)\n","            # Append the embedding to the list\n","            embeddings.append(image_embedding)\n","            ref_embedding.append(ref_image_embedding)\n","        # Stack the embeddings along a new dimension\n","        embeddings_tensor = torch.stack(embeddings, dim=1)\n","        ref_embedding_tensor = torch.stack(ref_embedding, dim=1)\n","        \n","        return embeddings_tensor.to(device), ref_embedding_tensor.to(device)\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T16:35:04.029979Z","iopub.status.busy":"2024-05-31T16:35:04.029597Z","iopub.status.idle":"2024-05-31T16:35:04.047487Z","shell.execute_reply":"2024-05-31T16:35:04.046321Z","shell.execute_reply.started":"2024-05-31T16:35:04.029948Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class PreferenceComparisonLoss(nn.Module):\n","    def __init__(self, margin=2.0):\n","        \"\"\"\n","        Preference Comparison Loss function for computing the loss between predicted\n","        and ground truth Gram matrices.\n","\n","        Args:\n","        - margin: Margin value for the loss calculation\n","        \"\"\"\n","        super(PreferenceComparisonLoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, output, label, ref):\n","        \"\"\"\n","        Forward pass of the Preference Comparison Loss function.\n","\n","        Args:\n","        - output: Predicted matrix from the model, shape (batch_size, n, embedding_dim)\n","        - ref: Reference matrix (ground truth), shape (batch_size, n, embedding_dim)\n","        - label: Ground truth labels, shape (batch_size, n, n)\n","\n","        Returns:\n","        - loss: Preference Comparison Loss\n","        \"\"\"\n","        # Calculate cosine similarity between output and ref matrices\n","        cosine_similarities = F.cosine_similarity(output, ref, dim=2)\n","        # Initialize loss variable\n","        loss_contrastive = 0\n","        count_pairs = 0\n","        # device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n","        device = torch.device(\"cpu\")\n","\n","        # Loop over each pair of images in the batch\n","        for i in range(output.size(0)):\n","            for j in range(output.size(1)):\n","                for k in range(output.size(1)):\n","                    if j != k:\n","                        # Get cosine similarity distances\n","                        cosine_distanceA = cosine_similarities[i, j]\n","                        cosine_distanceB = cosine_similarities[i, k]\n","\n","                        # Get label for the pair (j, k)\n","                        label_value = label[i, j, k].item()\n","                        # print(cosine_distanceA, type(cosine_distanceA), cosine_distanceB, type(cosine_distanceB), label_value, type(label_value))\n","                        # Calculate loss based on label\n","                        # if label_value < 2:\n","                        #     loss_contrastive += torch.pow(torch.clamp(torch.abs(cosine_distanceA - cosine_distanceB) + self.margin, min=0.0), 2)\n","                        #     count_pairs += 1\n","                        # else:\n","                        #     # Same difference\n","                        #     loss_contrastive +=\n","                        #  torch.pow(torch.clamp(self.margin - torch.abs(cosine_distanceA - cosine_distanceB), min=0.0), 2)\n","                        #     count_pairs += 1\n","                        if label_value < 2:\n","                            loss_contrastive += torch.nn.BCELoss()(torch.nn.Sigmoid()(cosine_distanceA - cosine_distanceB), torch.tensor(label_value, dtype=torch.float32).to(device))\n","                        else:\n","                            loss_contrastive += torch.pow(torch.clamp(self.margin - torch.abs(cosine_distanceA - cosine_distanceB), min=0.0), 2)\n","                        count_pairs += 1\n","        # Check if there are valid pairs to compute loss\n","        if count_pairs > 0:\n","            # Average loss across the valid pairs\n","            loss_contrastive /= count_pairs\n","        else:\n","            # If there are no valid pairs, return zero loss\n","            loss_contrastive = torch.tensor(0.0, requires_grad=True, device=output.device)\n","\n","        return loss_contrastive\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T16:35:04.049785Z","iopub.status.busy":"2024-05-31T16:35:04.049402Z","iopub.status.idle":"2024-05-31T16:35:04.080919Z","shell.execute_reply":"2024-05-31T16:35:04.079640Z","shell.execute_reply.started":"2024-05-31T16:35:04.049755Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torch.optim import lr_scheduler\n","\n","def train_model(model, train_dataset, val_dataset, checkpoint_folder, num_epochs=10, batch_size=32, learning_rate=0.001):\n","    \"\"\"\n","    Train the model using the provided datasets.\n","\n","    Args:\n","    - model: The model to be trained\n","    - train_dataset: Dataset for training\n","    - val_dataset: Dataset for validation\n","    - checkpoint_folder: Folder to store checkpoints\n","    - num_epochs: Number of epochs for training\n","    - batch_size: Batch size for training\n","    - learning_rate: Learning rate for optimization\n","\n","    Returns:\n","    - model: Trained model\n","    - train_losses: List of training losses\n","    - val_losses: List of validation losses\n","    \"\"\"\n","    # Create the checkpoint folder if it doesn't exist\n","    if not os.path.exists(checkpoint_folder):\n","        os.makedirs(checkpoint_folder)\n","    # device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n","    device = torch.device(\"cpu\")\n","    print(f\"Device: {device}\")\n","    # Define data loaders for training and validation\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Define loss function and optimizer\n","    criterion = PreferenceComparisonLoss()\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.0)\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","    # Lists to store training and validation losses\n","    train_losses = []\n","    val_losses = []\n","\n","    # Variables to keep track of the best model and its performance\n","    best_val_loss = float('inf')\n","    best_model_state = None\n","\n","    model = model.to(device)\n","    print(\"Training started...\")\n","    for epoch in range(num_epochs):\n","        print(\"*\"*100)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n","        model.train()\n","        running_train_loss = 0.0\n","        for i, (images, ref_images, gram_matrix, _) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            # Forward pass\n","            gram_matrix = gram_matrix.to(device)\n","            output, ref_output = model(images, ref_images, device)\n","            # Compute loss\n","            loss = criterion(output, gram_matrix,ref_output)\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","            running_train_loss += loss.item()\n","\n","            if i % 500 == 0:\n","                print(f\"\\t Batch [{i}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n","        \n","        # Compute average training loss for the epoch\n","        epoch_train_loss = running_train_loss / len(train_loader)\n","        train_losses.append(epoch_train_loss)\n","\n","        # Validation loop\n","        model.eval()\n","        running_val_loss = 0.0\n","        with torch.no_grad():\n","            for i, (images, ref_images, gram_matrix, _) in enumerate(val_loader):\n","                gram_matrix = gram_matrix.to(device)\n","                output, ref_output = model(images, ref_images, device)\n","                loss = criterion(output, gram_matrix,ref_output)\n","                running_val_loss += loss.item()\n","\n","                if i % 50 == 0:\n","                    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Batch [{i}/{len(val_loader)}], Val Loss: {loss.item():.4f}\")\n","        \n","        # Compute average validation loss for the epoch\n","        epoch_val_loss = running_val_loss / len(val_loader)\n","        val_losses.append(epoch_val_loss)\n","\n","        # Save the model checkpoint for every epoch (last model)\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_loss': epoch_val_loss\n","        }, os.path.join(checkpoint_folder, 'last.pt'))\n","\n","        # Save the best model checkpoint based on validation loss\n","        if epoch_val_loss < best_val_loss:\n","            best_val_loss = epoch_val_loss\n","            best_model_state = model.state_dict()\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': best_model_state,\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_loss': best_val_loss\n","            }, os.path.join(checkpoint_folder, 'best.pt'))\n","\n","        # Print progress\n","        print(f\"Validation, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n","        print(\"*\"*100)\n","        scheduler.step()\n","    print(\"Training completed.\")\n","\n","    return model, train_losses, val_losses\n","\n","\n","def test_model(model, test_dataset, batch_size=32):\n","    \"\"\"\n","    Evaluate the model on the test dataset.\n","\n","    Args:\n","    - model: The trained model to be evaluated\n","    - test_dataset: Dataset for testing\n","    - batch_size: Batch size for testing\n","\n","    Returns:\n","    - test_loss: Test loss\n","    \"\"\"\n","    # Define data loader for testing\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Define loss function\n","    criterion = nn.MSELoss()\n","\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Initialize variables for computing test loss\n","    running_test_loss = 0.0\n","    num_samples = 0\n","\n","    print(\"Testing started...\")\n","    with torch.no_grad():\n","        for images, labels in tqdm(test_loader):\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            running_test_loss += loss.item() * images.size(0)\n","            num_samples += images.size(0)\n","\n","    # Compute test loss\n","    test_loss = running_test_loss / num_samples\n","\n","    print(f\"Test Loss: {test_loss:.4f}\")\n","    print(\"Testing completed.\")\n","\n","    return test_loss"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T16:35:04.084498Z","iopub.status.busy":"2024-05-31T16:35:04.083731Z","iopub.status.idle":"2024-05-31T16:35:04.097555Z","shell.execute_reply":"2024-05-31T16:35:04.096341Z","shell.execute_reply.started":"2024-05-31T16:35:04.084449Z"},"trusted":true},"outputs":[],"source":["config = {\n","    \"annotation_data_path\": \"../csv/split_data.csv\",\n","    \"image_folder_path\": \"/media/jackson/Data/archive/Processed_Images\",\n","    \"model_encoder\": \"resnet50\",\n","    \"embedding_dim\": 512, \n","    \"learning_rate\": 1e-2,\n","    \"num_epoch\": 20,\n","    \"batch_size\": 1,\n","    \"num_per_cluster\": 5,\n","    \"num_groups\": 100,\n","    \"checkpoint_folder\": \"../weights_setting3/resnet50BasedModel\"\n","}"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T16:35:04.099685Z","iopub.status.busy":"2024-05-31T16:35:04.099157Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/jackson/anaconda3/envs/Paper/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/home/jackson/anaconda3/envs/Paper/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Device: cpu\n","Training started...\n","****************************************************************************************************\n","Epoch [1/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [1/20], Validation Batch [0/1], Val Loss: 0.6860\n","Validation, Train Loss: 0.6932, Val Loss: 0.6860\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [2/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [2/20], Validation Batch [0/1], Val Loss: 0.7070\n","Validation, Train Loss: 0.6931, Val Loss: 0.7070\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [3/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [3/20], Validation Batch [0/1], Val Loss: 0.6950\n","Validation, Train Loss: 0.6931, Val Loss: 0.6950\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [4/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [4/20], Validation Batch [0/1], Val Loss: 0.6930\n","Validation, Train Loss: 0.6932, Val Loss: 0.6930\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [5/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [5/20], Validation Batch [0/1], Val Loss: 0.6993\n","Validation, Train Loss: 0.6931, Val Loss: 0.6993\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [6/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [6/20], Validation Batch [0/1], Val Loss: 0.6940\n","Validation, Train Loss: 0.6931, Val Loss: 0.6940\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [7/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [7/20], Validation Batch [0/1], Val Loss: 0.7024\n","Validation, Train Loss: 0.6931, Val Loss: 0.7024\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [8/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [8/20], Validation Batch [0/1], Val Loss: 0.6991\n","Validation, Train Loss: 0.6931, Val Loss: 0.6991\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [9/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [9/20], Validation Batch [0/1], Val Loss: 0.7018\n","Validation, Train Loss: 0.6931, Val Loss: 0.7018\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [10/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [10/20], Validation Batch [0/1], Val Loss: 0.7015\n","Validation, Train Loss: 0.6931, Val Loss: 0.7015\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [11/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [11/20], Validation Batch [0/1], Val Loss: 0.7077\n","Validation, Train Loss: 0.6931, Val Loss: 0.7077\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [12/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [12/20], Validation Batch [0/1], Val Loss: 0.6920\n","Validation, Train Loss: 0.6931, Val Loss: 0.6920\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [13/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [13/20], Validation Batch [0/1], Val Loss: 0.6840\n","Validation, Train Loss: 0.6931, Val Loss: 0.6840\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [14/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [14/20], Validation Batch [0/1], Val Loss: 0.6899\n","Validation, Train Loss: 0.6931, Val Loss: 0.6899\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [15/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [15/20], Validation Batch [0/1], Val Loss: 0.6996\n","Validation, Train Loss: 0.6931, Val Loss: 0.6996\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [16/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [16/20], Validation Batch [0/1], Val Loss: 0.6952\n","Validation, Train Loss: 0.6931, Val Loss: 0.6952\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [17/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [17/20], Validation Batch [0/1], Val Loss: 0.6987\n","Validation, Train Loss: 0.6931, Val Loss: 0.6987\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [18/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [18/20], Validation Batch [0/1], Val Loss: 0.6850\n","Validation, Train Loss: 0.6931, Val Loss: 0.6850\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [19/20]:\n","\t Batch [0/100], Train Loss: 0.6932\n","Epoch [19/20], Validation Batch [0/1], Val Loss: 0.6969\n","Validation, Train Loss: 0.6931, Val Loss: 0.6969\n","****************************************************************************************************\n","****************************************************************************************************\n","Epoch [20/20]:\n","\t Batch [0/100], Train Loss: 0.6931\n","Epoch [20/20], Validation Batch [0/1], Val Loss: 0.6964\n","Validation, Train Loss: 0.6931, Val Loss: 0.6964\n","****************************************************************************************************\n","Training completed.\n"]},{"data":{"text/plain":["(Setting_3_model(\n","   (model): ResNet(\n","     (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","     (layer1): Sequential(\n","       (0): Bottleneck(\n","         (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","         (downsample): Sequential(\n","           (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","           (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         )\n","       )\n","       (1): Bottleneck(\n","         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","       (2): Bottleneck(\n","         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","     )\n","     (layer2): Sequential(\n","       (0): Bottleneck(\n","         (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","         (downsample): Sequential(\n","           (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","           (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         )\n","       )\n","       (1): Bottleneck(\n","         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","       (2): Bottleneck(\n","         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","       (3): Bottleneck(\n","         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","     )\n","     (layer3): Sequential(\n","       (0): Bottleneck(\n","         (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","         (downsample): Sequential(\n","           (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","           (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         )\n","       )\n","       (1): Bottleneck(\n","         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","       (2): Bottleneck(\n","         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","       (3): Bottleneck(\n","         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","       (4): Bottleneck(\n","         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","       (5): Bottleneck(\n","         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","     )\n","     (layer4): Sequential(\n","       (0): Bottleneck(\n","         (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","         (downsample): Sequential(\n","           (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","           (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         )\n","       )\n","       (1): Bottleneck(\n","         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","       (2): Bottleneck(\n","         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","         (relu): ReLU(inplace=True)\n","       )\n","     )\n","     (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","     (fc): Linear(in_features=2048, out_features=512, bias=True)\n","   )\n"," ),\n"," [0.6931602030992507,\n","  0.6931403446197509,\n","  0.6931491607427597,\n","  0.6931532323360443,\n","  0.693145654797554,\n","  0.6931439632177353,\n","  0.6931475478410721,\n","  0.6931388318538666,\n","  0.693139905333519,\n","  0.69314473092556,\n","  0.6931373262405396,\n","  0.693132312297821,\n","  0.6931393659114837,\n","  0.6931411755084992,\n","  0.6931459867954254,\n","  0.693139232993126,\n","  0.6931359219551086,\n","  0.6931345266103744,\n","  0.6931306517124176,\n","  0.6931249183416367],\n"," [0.6859963536262512,\n","  0.7070174813270569,\n","  0.6949602961540222,\n","  0.6930141448974609,\n","  0.6992813348770142,\n","  0.6940280199050903,\n","  0.7024273872375488,\n","  0.6990534067153931,\n","  0.7018406987190247,\n","  0.7014580368995667,\n","  0.7077429890632629,\n","  0.6919941902160645,\n","  0.6839612722396851,\n","  0.6899100542068481,\n","  0.6996456384658813,\n","  0.6951693296432495,\n","  0.6986979842185974,\n","  0.6849998831748962,\n","  0.6969321370124817,\n","  0.696398138999939])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset = SeveritySimilarityDataset(annotation_file_path=config[\"annotation_data_path\"],\n","                                    dataset_dir=config[\"image_folder_path\"],\n","                                    phase=\"training\",\n","                                    num_per_cluster=config[\"num_per_cluster\"],\n","                                    num_group = config[\"num_groups\"],\n","                                    input_size=(224, 224)\n","                                    )\n","\n","valid_dataset = SeveritySimilarityDataset(annotation_file_path=config[\"annotation_data_path\"],\n","                                    dataset_dir=config[\"image_folder_path\"],\n","                                    phase=\"valid\",\n","                                    num_per_cluster=config[\"num_per_cluster\"],\n","                                    num_group = config[\"num_groups\"]//100,\n","                                    input_size=(224, 224)\n","                                    )\n","model = Setting_3_model(model_name=config[\"model_encoder\"],\n","                        embed_dim=config[\"embedding_dim\"]\n","                        )\n","\n","\n","train_model(model=model, train_dataset=train_dataset,\n","            val_dataset=valid_dataset, num_epochs=config[\"num_epoch\"],\n","            batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n","            checkpoint_folder=config[\"checkpoint_folder\"]\n","            )"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5110964,"sourceId":8552654,"sourceType":"datasetVersion"},{"datasetId":5121102,"sourceId":8566029,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
